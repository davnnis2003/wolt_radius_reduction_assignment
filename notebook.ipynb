{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29593d68-667b-4aae-8394-0df37dabf9fd",
   "metadata": {},
   "source": [
    "# Overview of this Notebook and the approach\n",
    "\n",
    "## Toolings used\n",
    "* Python v3.11\n",
    "  * Pandas\n",
    "  * psycopg2\n",
    "  * sqlalchemy\n",
    "* Jupyter Notebook\n",
    "* PostgreSQL v16\n",
    "\n",
    "\n",
    "## The schema of the simple DWH (Data Warehouse) in PostgreSQL\n",
    "* `ods`: It stands for Operational Data Store, the very first landing area of the flat CSV files in the DWH\n",
    "* `int`: An intermediate layer for storing tables during the transformation process\n",
    "* `dim`: A delicate schema for Dimension tables. See [Dimensional modeling](https://en.wikipedia.org/wiki/Dimensional_modeling)\n",
    "* `fct`: A delicate schema for Fact tables.\n",
    "* `sum`: A delicate schema for Summary tables which contain pre-computed & pre-aggregated business metrics, consuming from tables in the `dim` & `fct` schemas only.\n",
    "\n",
    "### Data Ingestion\n",
    "It was done with Python ( Pandas, psycopg2, and sqlalchemy) and ingesting the 2 given CSV files (i.e. `purchases` & `delivery_radius_log`) into the `ods` schema in the PostgreSQL DWH.\n",
    "\n",
    "### Data Transformation\n",
    "The goal of the Data Transformation is usually creating tables (Data Marts) in the `dim`, `fct`, and `sum` schemas.\n",
    "\n",
    "For simple Data Marts (e.g. the Dimension Table of `delivery_areas`), it would directly consume from `ods`. If the transformation is complex, `int` is used to store the intermediate transformed tables.\n",
    "\n",
    "In addition, if there is a lot more data than the given task here, potentially an extra layer between the `ods`  and the `int` schema can be added to centralize all the cleaning logic. It can be named as [the \"Staging layer\"](https://medium.com/data-panda/dbt-models-staging-layer-55f0f2ddc5e4) like dbt Labs, or just the \"Data Layer\"/\"Merge Layer\" from a more old school practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea84d85-e39d-47ae-8eb3-ae8276b08d97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d25597f-b712-483a-b3d9-4d0439c59e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all packages used\n",
    "import pandas as pd\n",
    "import psycopg2 \n",
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6c4f0-701a-478d-a9c9-cb5a15bc143a",
   "metadata": {},
   "source": [
    "## Setting up a PostgreSQL DB  in localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67897c2-46e5-474b-84a1-a15e00ef2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "postgresql+psycopg2://dbuser:***@localhost/postgres"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a connection STRING for using a test service account `dbuser`\n",
    "from sqlalchemy import URL\n",
    "\n",
    "url_object = URL.create(\n",
    "    \"postgresql+psycopg2\",\n",
    "    username=\"dbuser\",\n",
    "    password=\"1\",  # plain (unescaped) text\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    ")\n",
    "\n",
    "url_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e64283-0dc6-4647-a5bd-ad6e0a9d1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the connection to the PostgreSQL instance, and create a test table there to confirm the connection works as expected\n",
    "db = create_engine(url_object) \n",
    "conn = db.connect() \n",
    "conn1 = psycopg2.connect( \n",
    "  database=\"postgres\", \n",
    "  user='dbuser',  \n",
    "  password='1',  \n",
    "  host='localhost',  \n",
    "  port= '5432'\n",
    ") \n",
    "  \n",
    "conn1.autocommit = True\n",
    "cursor = conn1.cursor() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed014d-f967-4f52-9462-7484fb5bfc9c",
   "metadata": {},
   "source": [
    "## Creating ODS tables in the local PostgreSQL server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d4872-70f6-4778-bb03-21f83040dc6d",
   "metadata": {},
   "source": [
    "### Importing delivery_radius_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ef486c-ee77-4b03-89fc-a6a72b76c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1316 entries, 0 to 1315\n",
      "Data columns (total 3 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   DELIVERY_AREA_ID         1316 non-null   object\n",
      " 1   DELIVERY_RADIUS_METERS   1316 non-null   int64 \n",
      " 2   EVENT_STARTED_TIMESTAMP  1316 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 31.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Importing the CSV file delivery_radius_log as a dataframe first\n",
    "df_delivery_radius_log = pd.read_csv('data/delivery_radius_log.csv')\n",
    "df_delivery_radius_log.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6051f81-8784-40f7-9a65-6e263b44ea51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DELIVERY_AREA_ID</th>\n",
       "      <th>DELIVERY_RADIUS_METERS</th>\n",
       "      <th>EVENT_STARTED_TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>3000</td>\n",
       "      <td>2022-06-14T08:26:20.923854Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>7000</td>\n",
       "      <td>2022-06-14T08:49:01.186365Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>3000</td>\n",
       "      <td>2022-06-18T07:43:57.662294Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>7000</td>\n",
       "      <td>2022-06-18T08:00:45.227506Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>4000</td>\n",
       "      <td>2022-06-18T08:05:29.093983Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DELIVERY_AREA_ID  DELIVERY_RADIUS_METERS  \\\n",
       "0  5db02e5d401d690c836b9ead                    3000   \n",
       "1  5db02e5d401d690c836b9ead                    7000   \n",
       "2  5db02e5d401d690c836b9ead                    3000   \n",
       "3  5db02e5d401d690c836b9ead                    7000   \n",
       "4  5d78a7e552dfabd5251dab7b                    4000   \n",
       "\n",
       "       EVENT_STARTED_TIMESTAMP  \n",
       "0  2022-06-14T08:26:20.923854Z  \n",
       "1  2022-06-14T08:49:01.186365Z  \n",
       "2  2022-06-18T07:43:57.662294Z  \n",
       "3  2022-06-18T08:00:45.227506Z  \n",
       "4  2022-06-18T08:05:29.093983Z  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exmaining the DataFrame\n",
    "df_delivery_radius_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a738f36-e501-4cb3-a8c7-3d5c8f1c2107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting the file into the PostgreSQL DB instance after checking it is okay\n",
    "df_delivery_radius_log.to_sql(\n",
    "    name = 'delivery_radius_log',\n",
    "    schema = 'ods',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a3b81-1e15-4902-9467-0b302f6e9ba5",
   "metadata": {},
   "source": [
    "### Importing purchases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d280eef0-22b9-4b71-8d8d-49d4daa700db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 177895 entries, 0 to 177894\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   PURCHASE_ID                            177895 non-null  object \n",
      " 1   TIME_RECEIVED                          177895 non-null  object \n",
      " 2   TIME_DELIVERED                         177895 non-null  object \n",
      " 3   END_AMOUNT_WITH_VAT_EUR                177895 non-null  float64\n",
      " 4   DROPOFF_DISTANCE_STRAIGHT_LINE_METRES  177895 non-null  int64  \n",
      " 5   DELIVERY_AREA_ID                       177895 non-null  object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Importing the CSV file delivery_radius_log as a dataframe first\n",
    "df_purchases = pd.read_csv('data/purchases.csv')\n",
    "df_purchases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09a6fe16-c2ef-411b-b02e-2110bf2173b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_ID</th>\n",
       "      <th>TIME_RECEIVED</th>\n",
       "      <th>TIME_DELIVERED</th>\n",
       "      <th>END_AMOUNT_WITH_VAT_EUR</th>\n",
       "      <th>DROPOFF_DISTANCE_STRAIGHT_LINE_METRES</th>\n",
       "      <th>DELIVERY_AREA_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f85beff7762a1539ad6faf1</td>\n",
       "      <td>2022-10-13T14:51:43.048Z</td>\n",
       "      <td>2022-10-13T15:18:35.265Z</td>\n",
       "      <td>17.87</td>\n",
       "      <td>735</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f85c08dddf0c9826389f3cd</td>\n",
       "      <td>2022-10-13T14:58:21.078Z</td>\n",
       "      <td>2022-10-13T15:28:09.194Z</td>\n",
       "      <td>17.75</td>\n",
       "      <td>436</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f85bc2cf49ddea98955ce5f</td>\n",
       "      <td>2022-10-13T14:39:40.153Z</td>\n",
       "      <td>2022-10-13T15:05:15.058Z</td>\n",
       "      <td>25.80</td>\n",
       "      <td>867</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f855dbf5a93deaf2be5b872</td>\n",
       "      <td>2022-10-13T07:56:47.003Z</td>\n",
       "      <td>2022-10-13T09:05:14.37Z</td>\n",
       "      <td>15.70</td>\n",
       "      <td>252</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5f85be8a8876393ee141ed82</td>\n",
       "      <td>2022-10-13T14:49:46.693Z</td>\n",
       "      <td>2022-10-13T15:14:31.299Z</td>\n",
       "      <td>18.80</td>\n",
       "      <td>857</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PURCHASE_ID             TIME_RECEIVED  \\\n",
       "0  5f85beff7762a1539ad6faf1  2022-10-13T14:51:43.048Z   \n",
       "1  5f85c08dddf0c9826389f3cd  2022-10-13T14:58:21.078Z   \n",
       "2  5f85bc2cf49ddea98955ce5f  2022-10-13T14:39:40.153Z   \n",
       "3  5f855dbf5a93deaf2be5b872  2022-10-13T07:56:47.003Z   \n",
       "4  5f85be8a8876393ee141ed82  2022-10-13T14:49:46.693Z   \n",
       "\n",
       "             TIME_DELIVERED  END_AMOUNT_WITH_VAT_EUR  \\\n",
       "0  2022-10-13T15:18:35.265Z                    17.87   \n",
       "1  2022-10-13T15:28:09.194Z                    17.75   \n",
       "2  2022-10-13T15:05:15.058Z                    25.80   \n",
       "3   2022-10-13T09:05:14.37Z                    15.70   \n",
       "4  2022-10-13T15:14:31.299Z                    18.80   \n",
       "\n",
       "   DROPOFF_DISTANCE_STRAIGHT_LINE_METRES          DELIVERY_AREA_ID  \n",
       "0                                    735  5d78a7e552dfabd5251dab7b  \n",
       "1                                    436  5cc1b60b034adf90cd8f14dd  \n",
       "2                                    867  5cc1b60b034adf90cd8f14dd  \n",
       "3                                    252  5db02e5d401d690c836b9ead  \n",
       "4                                    857  5db02e5d401d690c836b9ead  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exmining the DataFrame example data\n",
    "df_purchases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3fa0027-439f-472c-ad1b-02d10c1b7b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting the DataFrame into the PostgreSQL DB instance\n",
    "df_purchases.to_sql(\n",
    "    name = 'purchases',\n",
    "    schema = 'ods',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255aac7-5ef7-4585-af89-ceb7f6929a05",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "In the first task youâ€™ll work with the delivery radius log dataset. Given this delivery radius change log, we would like you to detect at any given time what is a temporary reduction (or increase) of the delivery radius and what is the \"default\" (more permanent) delivery radius. For this exercise, you can assume that the default radius at any given time is a radius that has lasted for at least 24 hours uninterrupted.\n",
    "\n",
    "We would like you to produce a dataset(s) and answer the following:\n",
    "* What are all the default delivery radiuses for the delivery areas during the timeframe\n",
    "provided? Keep in mind that each area can have multiple default radiuses in the given\n",
    "dataset.\n",
    "* How many hours of radius reductions with respect to the the default radiuses have we\n",
    "had during the timeframe provided for each delivery area?\n",
    "\n",
    "Please give answers in numerical values to the above questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da09a10-8b89-4b6e-be00-9577c0b6bfb4",
   "metadata": {},
   "source": [
    "## Task 1 Interpretation\n",
    "\n",
    "Given the `delivery_radius_log` file is basically Delivery Radius changing events (both of delivery radius expansion & reduction), and the defination of the Default Delivery Radius, this task needs to be solved by 2 steps:\n",
    "1. Build a CTE that would only include records lasting longer than 24 hours (i.e. a table containing only changelog of Default Delivery Radius changes); Then,\n",
    "2. Depends on the CTE from 1, calculate the temporary radius reductions since the latest change of the Default Delivery Radius (and until the next change of the Delivery Radius).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "459a6ef2-b157-4836-b2af-671fa93c875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>current_default_radius_event_started_timestamp</th>\n",
       "      <th>previous_default_radius_event_started_timestamp</th>\n",
       "      <th>next_default_radius_event_started_timestamp</th>\n",
       "      <th>default_delivery_radius_meters</th>\n",
       "      <th>delivery_radius_duration_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>1900-01-01 00:00:00.000000</td>\n",
       "      <td>2021-12-02 13:27:00.815321+01:00</td>\n",
       "      <td>6500</td>\n",
       "      <td>24.769979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>2021-12-05 16:11:52.970808+01:00</td>\n",
       "      <td>6500</td>\n",
       "      <td>74.431627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-05 16:11:52.970808</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>2021-12-08 14:07:17.814469+01:00</td>\n",
       "      <td>6500</td>\n",
       "      <td>69.759161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-08 14:07:17.814469</td>\n",
       "      <td>2021-12-05 16:11:52.970808</td>\n",
       "      <td>2021-12-10 13:28:13.172008+01:00</td>\n",
       "      <td>6500</td>\n",
       "      <td>47.199525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-10 13:28:13.172008</td>\n",
       "      <td>2021-12-08 14:07:17.814469</td>\n",
       "      <td>2021-12-11 14:34:38.885918+01:00</td>\n",
       "      <td>6500</td>\n",
       "      <td>24.872071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id current_default_radius_event_started_timestamp  \\\n",
       "0  5cc1b60b034adf90cd8f14dd                     2021-12-01 12:30:09.405860   \n",
       "1  5cc1b60b034adf90cd8f14dd                     2021-12-02 13:27:00.815321   \n",
       "2  5cc1b60b034adf90cd8f14dd                     2021-12-05 16:11:52.970808   \n",
       "3  5cc1b60b034adf90cd8f14dd                     2021-12-08 14:07:17.814469   \n",
       "4  5cc1b60b034adf90cd8f14dd                     2021-12-10 13:28:13.172008   \n",
       "\n",
       "  previous_default_radius_event_started_timestamp  \\\n",
       "0                      1900-01-01 00:00:00.000000   \n",
       "1                      2021-12-01 12:30:09.405860   \n",
       "2                      2021-12-02 13:27:00.815321   \n",
       "3                      2021-12-05 16:11:52.970808   \n",
       "4                      2021-12-08 14:07:17.814469   \n",
       "\n",
       "  next_default_radius_event_started_timestamp  default_delivery_radius_meters  \\\n",
       "0            2021-12-02 13:27:00.815321+01:00                            6500   \n",
       "1            2021-12-05 16:11:52.970808+01:00                            6500   \n",
       "2            2021-12-08 14:07:17.814469+01:00                            6500   \n",
       "3            2021-12-10 13:28:13.172008+01:00                            6500   \n",
       "4            2021-12-11 14:34:38.885918+01:00                            6500   \n",
       "\n",
       "   delivery_radius_duration_hours  \n",
       "0                       24.769979  \n",
       "1                       74.431627  \n",
       "2                       69.759161  \n",
       "3                       47.199525  \n",
       "4                       24.872071  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_fct_delivery_areas_default_radius_events = '''\n",
    "WITH ods_delivery_radius_log AS (\n",
    "\t-- PostgreSQL syntax issue, need to use `\"` to specify the exact column name\n",
    "\tSELECT \"DELIVERY_AREA_ID\" AS delivery_area_id\n",
    "\t\t, \"EVENT_STARTED_TIMESTAMP\"::TIMESTAMP AS event_started_timestamp\n",
    "\t\t, \"DELIVERY_RADIUS_METERS\" AS delivery_radius_meters\n",
    "\tFROM ods.delivery_radius_log \n",
    "), delivery_radius_log AS (\n",
    "    SELECT *\n",
    "        , LAG(delivery_radius_meters) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp) AS previous_delivery_radius_meters\n",
    "        , LAG(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp) AS previous_event_started_timestamp \n",
    "        , LEAD(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp) AS next_event_started_timestamp \n",
    "    FROM ods_delivery_radius_log\n",
    "), fct_delivery_radius_log AS (\n",
    "    SELECT *\n",
    "      , EXTRACT('epoch' FROM (next_event_started_timestamp - event_started_timestamp))/3600 AS delivery_radius_duration_hours\n",
    "    FROM delivery_radius_log\n",
    "), main_query AS (\n",
    "\tSELECT *\n",
    "\t\t, (delivery_radius_duration_hours >= 24) AS is_default_delivery_radius\n",
    "\tFROM fct_delivery_radius_log\n",
    ")\n",
    "SELECT delivery_area_id\n",
    "\t, event_started_timestamp AS current_default_radius_event_started_timestamp\n",
    "\t, COALESCE(LAG(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp), '1900-01-01 00:00:00') AS previous_default_radius_event_started_timestamp \n",
    "    , COALESCE(LEAD(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp), CURRENT_TIMESTAMP) AS next_default_radius_event_started_timestamp \n",
    "\t, delivery_radius_meters AS default_delivery_radius_meters\n",
    "    , delivery_radius_duration_hours\n",
    "FROM main_query\n",
    "WHERE is_default_delivery_radius\n",
    "ORDER BY delivery_area_id, event_started_timestamp\n",
    "'''\n",
    "\n",
    "df_fct_delivery_areas_default_radius_events = pd.read_sql(sql_fct_delivery_areas_default_radius_events, con = db)\n",
    "\n",
    "df_fct_delivery_areas_default_radius_events.to_sql(\n",
    "    name = 'delivery_areas_default_radius_events',\n",
    "    schema = 'fct',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_fct_delivery_areas_default_radius_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39d45afd-6d40-4b7c-9a9d-1caf65fe29c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>event_started_timestamp</th>\n",
       "      <th>delivery_radius_duration_hours</th>\n",
       "      <th>default_delivery_radius_meters</th>\n",
       "      <th>delivery_radius_meters</th>\n",
       "      <th>is_reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-01 12:12:41.947087</td>\n",
       "      <td>0.290961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3500</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>24.769979</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>6500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-02 13:16:21.329693</td>\n",
       "      <td>0.177635</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>3500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>74.431627</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>6500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-05 15:52:54.673552</td>\n",
       "      <td>0.316194</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>3500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id    event_started_timestamp  \\\n",
       "0  5cc1b60b034adf90cd8f14dd 2021-12-01 12:12:41.947087   \n",
       "1  5cc1b60b034adf90cd8f14dd 2021-12-01 12:30:09.405860   \n",
       "2  5cc1b60b034adf90cd8f14dd 2021-12-02 13:16:21.329693   \n",
       "3  5cc1b60b034adf90cd8f14dd 2021-12-02 13:27:00.815321   \n",
       "4  5cc1b60b034adf90cd8f14dd 2021-12-05 15:52:54.673552   \n",
       "\n",
       "   delivery_radius_duration_hours  default_delivery_radius_meters  \\\n",
       "0                        0.290961                             NaN   \n",
       "1                       24.769979                          6500.0   \n",
       "2                        0.177635                          6500.0   \n",
       "3                       74.431627                          6500.0   \n",
       "4                        0.316194                          6500.0   \n",
       "\n",
       "   delivery_radius_meters is_reduction  \n",
       "0                    3500         None  \n",
       "1                    6500        False  \n",
       "2                    3500         True  \n",
       "3                    6500        False  \n",
       "4                    3500         True  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_fct_delivery_radius_events = '''\n",
    "WITH delivery_radius_log AS (\n",
    "    SELECT *\n",
    "        , LAG(delivery_radius_meters) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp) AS previous_delivery_radius_meters\n",
    "        , COALESCE(LAG(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp), '1900-01-01 00:00:00') AS previous_event_started_timestamp \n",
    "        , COALESCE(LEAD(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp), CURRENT_TIMESTAMP) AS next_event_started_timestamp \n",
    "    FROM (\n",
    "        -- PostgreSQL syntax issue, need to use `\"` to specify the exact column name\n",
    "        SELECT \"DELIVERY_AREA_ID\" AS delivery_area_id\n",
    "            , \"EVENT_STARTED_TIMESTAMP\"::TIMESTAMP AS event_started_timestamp\n",
    "            , \"DELIVERY_RADIUS_METERS\" AS delivery_radius_meters\n",
    "        FROM ods.delivery_radius_log \n",
    "    )\n",
    "), fct_delivery_radius_log AS (\n",
    "    SELECT *\n",
    "      , EXTRACT('epoch' FROM (next_event_started_timestamp - event_started_timestamp))/3600 AS delivery_radius_duration_hours\n",
    "    FROM delivery_radius_log\n",
    "), main_query AS (\n",
    "\tSELECT events.*\n",
    "\t\t, CASE\n",
    "\t\t\tWHEN events.delivery_radius_duration_hours >= 24\n",
    "\t\t\t\tTHEN events.delivery_radius_meters\n",
    "\t\t\tELSE default_events.default_delivery_radius_meters\n",
    "\t\t  END AS default_delivery_radius_meters\n",
    "\tFROM fct_delivery_radius_log events\n",
    "\tLEFT JOIN fct.delivery_areas_default_radius_events default_events ON events.delivery_area_id = default_events.delivery_area_id\n",
    "\t\tAND events.event_started_timestamp > default_events.current_default_radius_event_started_timestamp \n",
    "\t\tAND events.event_started_timestamp <= default_events.next_default_radius_event_started_timestamp\n",
    ")\n",
    "SELECT delivery_area_id\n",
    "\t, event_started_timestamp\n",
    "\t, delivery_radius_duration_hours\n",
    "\t, default_delivery_radius_meters\n",
    "\t, delivery_radius_meters\n",
    "\t, (default_delivery_radius_meters > delivery_radius_meters) AS is_reduction\n",
    "FROM main_query\n",
    "ORDER BY delivery_area_id, event_started_timestamp\n",
    "'''\n",
    "\n",
    "df_fct_delivery_radius_events = pd.read_sql(sql_fct_delivery_radius_events, con = db)\n",
    "\n",
    "df_fct_delivery_radius_events.to_sql(\n",
    "    name = 'delivery_radius_events',\n",
    "    schema = 'fct',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_fct_delivery_radius_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bcdf75-207d-4810-bdfe-8699eef748f2",
   "metadata": {},
   "source": [
    "* The output table `delivery_areas_default_radius_events` is a changelog of per Delivery Areas (`delivery_area_id`) with their default Delivery Radius (i.e. `default_delivery_radius_meters`) over time.\n",
    "  * Given the dynamic nature of the Delivery Area radius, it is better to deliver a Fact table instead of a Dimension table (or Summary table).\n",
    "* A simple query can be ran on top of `fct.delivery_radius_events` to calculate the hours of radius reductions with respect to the default radiuses, e.g.\n",
    "```sql\n",
    "SELECT delivery_area_id\n",
    "\t, event_started_timestamp\n",
    "\t, delivery_radius_duration_hours\n",
    "FROM fct.delivery_radius_events\n",
    "WHERE is_reduction\n",
    "```\n",
    "\n",
    "* Important Note: The current query is not perfect per se and could use some refactoring for when the dataset grows bigger in the future. If it is on proper DWH solutions (e.g. Google BigQuery, Snowflake etc.), using Windows Function with IGNORE NULLS would be a lot more scalable way to do so compared to JOINs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69e08e6c-26a6-40fa-913a-c440da39c510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>event_started_timestamp</th>\n",
       "      <th>delivery_radius_duration_hours</th>\n",
       "      <th>default_delivery_radius_meters</th>\n",
       "      <th>delivery_radius_meters</th>\n",
       "      <th>is_reduction</th>\n",
       "      <th>previous_event_started_timestamp</th>\n",
       "      <th>next_event_started_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-01 12:12:41.947087</td>\n",
       "      <td>0.290961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3500</td>\n",
       "      <td>None</td>\n",
       "      <td>1900-01-01 00:00:00.000000</td>\n",
       "      <td>2021-12-01 12:30:09.405860+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>24.769979</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>6500</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-12-01 12:12:41.947087</td>\n",
       "      <td>2021-12-02 13:16:21.329693+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-02 13:16:21.329693</td>\n",
       "      <td>0.177635</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>3500</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>2021-12-02 13:27:00.815321+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>74.431627</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>6500</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-12-02 13:16:21.329693</td>\n",
       "      <td>2021-12-05 15:52:54.673552+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-05 15:52:54.673552</td>\n",
       "      <td>0.316194</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>3500</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>2021-12-05 16:11:52.970808+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id    event_started_timestamp  \\\n",
       "0  5cc1b60b034adf90cd8f14dd 2021-12-01 12:12:41.947087   \n",
       "1  5cc1b60b034adf90cd8f14dd 2021-12-01 12:30:09.405860   \n",
       "2  5cc1b60b034adf90cd8f14dd 2021-12-02 13:16:21.329693   \n",
       "3  5cc1b60b034adf90cd8f14dd 2021-12-02 13:27:00.815321   \n",
       "4  5cc1b60b034adf90cd8f14dd 2021-12-05 15:52:54.673552   \n",
       "\n",
       "   delivery_radius_duration_hours  default_delivery_radius_meters  \\\n",
       "0                        0.290961                             NaN   \n",
       "1                       24.769979                          6500.0   \n",
       "2                        0.177635                          6500.0   \n",
       "3                       74.431627                          6500.0   \n",
       "4                        0.316194                          6500.0   \n",
       "\n",
       "   delivery_radius_meters is_reduction previous_event_started_timestamp  \\\n",
       "0                    3500         None       1900-01-01 00:00:00.000000   \n",
       "1                    6500        False       2021-12-01 12:12:41.947087   \n",
       "2                    3500         True       2021-12-01 12:30:09.405860   \n",
       "3                    6500        False       2021-12-02 13:16:21.329693   \n",
       "4                    3500         True       2021-12-02 13:27:00.815321   \n",
       "\n",
       "       next_event_started_timestamp  \n",
       "0  2021-12-01 12:30:09.405860+01:00  \n",
       "1  2021-12-02 13:16:21.329693+01:00  \n",
       "2  2021-12-02 13:27:00.815321+01:00  \n",
       "3  2021-12-05 15:52:54.673552+01:00  \n",
       "4  2021-12-05 16:11:52.970808+01:00  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_radius_reduction = '''\n",
    "SELECT delivery_area_id\n",
    "\t, event_started_timestamp\n",
    "\t, delivery_radius_duration_hours\n",
    "FROM fct.delivery_radius_events\n",
    "WHERE is_reduction\n",
    "'''\n",
    "\n",
    "df_fct_delivery_radius_events = pd.read_sql(sql_fct_delivery_radius_events, con = db)\n",
    "\n",
    "df_fct_delivery_radius_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe280c-24d6-4017-9549-48b156082ada",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Now that we know the default delivery radiuses and times when the delivery radius was reduced, we would like you to create a derived dataset aggregated to hourly level that can be used to analyze delivery radius reductions and purchases in the areas for any hour in 2022. Build the dataset so that anyone could query the data without writing further joins or calculations and would be able to answer the following questions with a simple SELECT statement:\n",
    "* How many purchases and how much revenue (End Amount With VAT Eur) do we produce during the hour?\n",
    "* How long do the deviations (reductions) from default radius last during the hour? How many times have we modified the radius during the hour?\n",
    "* How do these hourly values compare to the previous week for each area? This is just a simple week-over-week percentage difference for each of the above-mentioned four measures.\n",
    "\n",
    "We want to emphasize that all three questions should be answered with the same aggregated dataset, meaning for instance that even the week-over-week differences are pre-calculated. Please note that for this task it is enough to only create the dataset and you are not expected to answer these questions. We only wish to see the code which creates this table and a sample of a few rows from the resulting dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba882c-0b9a-4a27-8cfb-9097e2dd9494",
   "metadata": {},
   "source": [
    "## Task 2 Interpretation\n",
    "\n",
    "From the requirement of Task 2, it is obvious that a Summary table needs to be built. The spec of this Summary table would be:\n",
    "1. Aggregated to the **hourly level**. Along with the Delivery Area information, it means the combined key of the `base_hour` + `delivery_area_id` columns should be there as the basic dimensions.\n",
    "2. As for Metrics, these columns would have to be there as well:\n",
    "    * Sales related: Purchases & Revenue (End Amount with Vat Eur). They\n",
    "    * Delivery Radius related: Reduction duration & the frequency of delivery radius changing events (both expansion & reduction)\n",
    "      * Also, it is assumed that all changes in the Delivery Radius are to be included (regardless if they are manually modifying events or automatically triggered)\n",
    "3. On top of all 4 Metrics mentioned in point 2, WoW (Week-over-week) difference in percentage (%) would also need to be derived from them\n",
    "    * It is also assumed that the WoW comparison window would be the same Day of the Week **and** same Hour of the Day, e.g. the number of Purchases from this Monday 1800 - 1900 would only be compared to the number of Purchases from last Monday 1800 - 1900.\n",
    "\n",
    "\n",
    "From the technical perspective, here is the outline of the steps to take to build the Summary table\n",
    "1. Preparation steps\n",
    "    1. Generate a Fact table of Purchases\n",
    "    2. Generate a Dimension table of all Delivery Areas\n",
    "2. Development of intermediate tables\n",
    "    1. Build the bases:\n",
    "        1. Base Hours: Build an intermediate table containing all hours from 1 Jan 2022 till 31 Dec 2022\n",
    "        2. Base Hours with all Delivery Areas: Get all possible combinations of Base Hours & Delivery Areas with a CROSS JOIN\n",
    "    2. Calculate the aggregated Sales Metrics (i.e. purchases and revenue) per hour per Delivery Area\n",
    "    3. Calculate the aggregated Delivery Radius Metrics\n",
    "        1. Calculate the aggregated Reduction duration per hour per Delivery Area\n",
    "        2. Calculate the number of delivery radius changing events per hour per Delivery Area\n",
    "3. Putting everything from step 2 together, by:\n",
    "    * Using the base table from 2A as the base, then\n",
    "    * LEFT JOIN the aggregated table from 2B to fetch purchases & revenue\n",
    "    * LEFT JOIN the aggregated table from 2C to fetch reduction duration & revenue number of delivery radius changing events\n",
    "4. Based on the table from step 3, calculate the WoW percentage difference\n",
    "    * COALESCE function would need to be used to replace NULLs with 0 if no metrics are found for that particular window\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013c015-f4f4-49fb-9fbe-ed35212650c4",
   "metadata": {},
   "source": [
    "## Prep work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ba871-3ac2-47dc-b48a-4180acd4a4ff",
   "metadata": {},
   "source": [
    "### Generate a Fact table of Purchases\n",
    "\n",
    "The structure of the ODS table of `purchases` isn't good enough and need some enrichment with new columns, and they will be vital for later transformation. This step is to build a proper Fact table based on it and enforce the data type per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2634a8f3-18ba-4d1a-aa90-3f6d537a478b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "      <th>time_received</th>\n",
       "      <th>time_delivered</th>\n",
       "      <th>end_amount_with_vat_eur</th>\n",
       "      <th>dropoff_distance_straight_line_metres</th>\n",
       "      <th>delivery_area_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e0bf2fdc3cded7e3ee098ae</td>\n",
       "      <td>2022-01-01 01:16:45.557</td>\n",
       "      <td>2022-01-02 09:36:24.341</td>\n",
       "      <td>28.90</td>\n",
       "      <td>362</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e0c53153040aacc8319ebd7</td>\n",
       "      <td>2022-01-01 08:06:45.992</td>\n",
       "      <td>2022-01-01 08:25:41.983</td>\n",
       "      <td>30.05</td>\n",
       "      <td>454</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e0c55e571bf4d9b33e0a3c0</td>\n",
       "      <td>2022-01-01 08:18:45.382</td>\n",
       "      <td>2022-01-01 08:44:36.997</td>\n",
       "      <td>18.70</td>\n",
       "      <td>1288</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e0c56163040aacc8319ef65</td>\n",
       "      <td>2022-01-01 08:19:34.633</td>\n",
       "      <td>2022-01-01 08:51:02.019</td>\n",
       "      <td>32.00</td>\n",
       "      <td>2044</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e0c577d93a422bd1b6dc02d</td>\n",
       "      <td>2022-01-01 08:25:33.776</td>\n",
       "      <td>2022-01-01 08:56:42.016</td>\n",
       "      <td>39.20</td>\n",
       "      <td>2414</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                purchase_id           time_received          time_delivered  \\\n",
       "0  5e0bf2fdc3cded7e3ee098ae 2022-01-01 01:16:45.557 2022-01-02 09:36:24.341   \n",
       "1  5e0c53153040aacc8319ebd7 2022-01-01 08:06:45.992 2022-01-01 08:25:41.983   \n",
       "2  5e0c55e571bf4d9b33e0a3c0 2022-01-01 08:18:45.382 2022-01-01 08:44:36.997   \n",
       "3  5e0c56163040aacc8319ef65 2022-01-01 08:19:34.633 2022-01-01 08:51:02.019   \n",
       "4  5e0c577d93a422bd1b6dc02d 2022-01-01 08:25:33.776 2022-01-01 08:56:42.016   \n",
       "\n",
       "   end_amount_with_vat_eur  dropoff_distance_straight_line_metres  \\\n",
       "0                    28.90                                    362   \n",
       "1                    30.05                                    454   \n",
       "2                    18.70                                   1288   \n",
       "3                    32.00                                   2044   \n",
       "4                    39.20                                   2414   \n",
       "\n",
       "           delivery_area_id  \n",
       "0  5db02e5d401d690c836b9ead  \n",
       "1  5db02e5d401d690c836b9ead  \n",
       "2  5db02e5d401d690c836b9ead  \n",
       "3  5cc1b60b034adf90cd8f14dd  \n",
       "4  5cc1b60b034adf90cd8f14dd  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_fct_purchases = '''\n",
    "SELECT \"PURCHASE_ID\" AS purchase_id\n",
    "\t, \"TIME_RECEIVED\"::TIMESTAMP AS time_received\n",
    "\t, \"TIME_DELIVERED\"::TIMESTAMP AS time_delivered\n",
    "\t, \"END_AMOUNT_WITH_VAT_EUR\" AS end_amount_with_vat_eur\n",
    "\t, \"DROPOFF_DISTANCE_STRAIGHT_LINE_METRES\" AS dropoff_distance_straight_line_metres\n",
    "\t, \"DELIVERY_AREA_ID\" AS delivery_area_id\n",
    "FROM ods.purchases\n",
    "ORDER BY time_received\n",
    "\n",
    "'''\n",
    "\n",
    "df_fct_purchases = pd.read_sql(sql_fct_purchases, con = db)\n",
    "\n",
    "df_fct_purchases.to_sql(\n",
    "    name = 'purchases',\n",
    "    schema = 'fct',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_fct_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25014a-e0db-4a88-8f6d-f598216929d1",
   "metadata": {},
   "source": [
    "### Generate a Dimension table for all Delivery Areas\n",
    "\n",
    "Although the Fact table of Default Delivery Radius is there already, it is still useful to have a Dimension table having `delivery_area_id` as the Primary Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edd8f1bf-6fdc-43ae-a94b-2e30a214ff96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id\n",
       "0  5cc1b60b034adf90cd8f14dd\n",
       "1  5db02e5d401d690c836b9ead\n",
       "2  5d78a7e552dfabd5251dab7b"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dim_delivery_areas = '''\n",
    "SELECT DISTINCT \"DELIVERY_AREA_ID\" AS delivery_area_id\n",
    "FROM ods.delivery_radius_log \n",
    ";\n",
    "'''\n",
    "\n",
    "df_dim_delivery_areas = pd.read_sql(sql_dim_delivery_areas, con = db)\n",
    "\n",
    "df_dim_delivery_areas.to_sql(\n",
    "    name = 'delivery_areas',\n",
    "    schema = 'dim',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_dim_delivery_areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cca4c4-f577-458d-a68a-a6eb458bc523",
   "metadata": {},
   "source": [
    "## Development of queries for the required dataset in Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c9985-7248-4b81-871a-0135df5aaed4",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Base Hours\n",
    "\n",
    "It needs to be on a hourly basis as it is the very base of the required Summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb927c4e-09ba-4557-8a06-7193cc449876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   base_hour\n",
       "0  2022-01-01 00:00:00+01:00\n",
       "1  2022-01-01 01:00:00+01:00\n",
       "2  2022-01-01 02:00:00+01:00\n",
       "3  2022-01-01 03:00:00+01:00\n",
       "4  2022-01-01 04:00:00+01:00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_base_hours = '''\n",
    "SELECT GENERATE_SERIES(date '2022-01-01', date '2022-12-31', '1 hour') AS base_hour\n",
    ";\n",
    "'''\n",
    "\n",
    "df_base_hours = pd.read_sql(sql_base_hours, con = db)\n",
    "\n",
    "df_base_hours.to_sql(\n",
    "    name = 'base_hours',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_base_hours.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1034c-e46f-4116-8681-f2d5de7cb42a",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Base Hours with all Delivery Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c44f0e2-45c5-4b9a-96a0-cafcc4436e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_hour</th>\n",
       "      <th>delivery_area_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00+01:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00+01:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00+01:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00+01:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00+01:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   base_hour          delivery_area_id\n",
       "0  2022-01-01 00:00:00+01:00  5cc1b60b034adf90cd8f14dd\n",
       "1  2022-01-01 01:00:00+01:00  5cc1b60b034adf90cd8f14dd\n",
       "2  2022-01-01 02:00:00+01:00  5cc1b60b034adf90cd8f14dd\n",
       "3  2022-01-01 03:00:00+01:00  5cc1b60b034adf90cd8f14dd\n",
       "4  2022-01-01 04:00:00+01:00  5cc1b60b034adf90cd8f14dd"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop the Summary of Sales\n",
    "sql_base_hours_with_delivery_areas = '''\n",
    "WITH base_hours AS (\n",
    "    SELECT generate_series(date '2022-01-01', date '2022-12-31', '1 hour') AS base_hour\n",
    ")\n",
    "SELECT base_hours.base_hour\n",
    "    , delivery_areas.delivery_area_id\n",
    "FROM base_hours\n",
    "CROSS JOIN dim.delivery_areas\n",
    ";\n",
    "'''\n",
    "\n",
    "df_base_hours_with_delivery_areas = pd.read_sql(sql_base_hours_with_delivery_areas, con = db)\n",
    "\n",
    "df_base_hours_with_delivery_areas.to_sql(\n",
    "    name = 'base_hours_with_delivery_areas',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_base_hours_with_delivery_areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537e9ba-2daa-47e9-9c36-b03cf42807cc",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Purchaes Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6af78b27-dc03-434b-a944-cde108d4d7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>base_hour</th>\n",
       "      <th>nb_purchases</th>\n",
       "      <th>end_amount_with_vat_eur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>104.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 09:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>129.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 10:00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>626.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 11:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>515.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 12:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>573.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id           base_hour  nb_purchases  \\\n",
       "0  5cc1b60b034adf90cd8f14dd 2022-01-01 08:00:00             3   \n",
       "1  5cc1b60b034adf90cd8f14dd 2022-01-01 09:00:00             4   \n",
       "2  5cc1b60b034adf90cd8f14dd 2022-01-01 10:00:00            22   \n",
       "3  5cc1b60b034adf90cd8f14dd 2022-01-01 11:00:00            15   \n",
       "4  5cc1b60b034adf90cd8f14dd 2022-01-01 12:00:00            19   \n",
       "\n",
       "   end_amount_with_vat_eur  \n",
       "0                   104.65  \n",
       "1                   129.30  \n",
       "2                   626.70  \n",
       "3                   515.00  \n",
       "4                   573.75  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop the Summary of Sales\n",
    "sql_sum_purchases = '''\n",
    "SELECT delivery_area_id\n",
    "    , DATE_TRUNC('HOUR', time_received) AS base_hour\n",
    "    , COUNT(*) AS nb_purchases\n",
    "    , SUM(end_amount_with_vat_eur) AS end_amount_with_vat_eur\n",
    "FROM fct.purchases\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2\n",
    ";\n",
    "'''\n",
    "\n",
    "df_sum_purchases = pd.read_sql(sql_sum_purchases, con = db)\n",
    "\n",
    "df_sum_purchases.to_sql(\n",
    "    name = 'sum_purchases',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_sum_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08901f-0c56-4bb4-b8d3-32993c248fb5",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Delivery Radius Reduction summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "177ae3fb-6c08-4912-92f8-03c3a067ea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>base_hour</th>\n",
       "      <th>delta_hours_radius_reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id           base_hour  delta_hours_radius_reduction\n",
       "0  5cc1b60b034adf90cd8f14dd 2022-01-01 00:00:00                           0.0\n",
       "1  5cc1b60b034adf90cd8f14dd 2022-01-01 01:00:00                           0.0\n",
       "2  5cc1b60b034adf90cd8f14dd 2022-01-01 02:00:00                           0.0\n",
       "3  5cc1b60b034adf90cd8f14dd 2022-01-01 03:00:00                           0.0\n",
       "4  5cc1b60b034adf90cd8f14dd 2022-01-01 04:00:00                           0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_sum_delivery_radius_reduction = '''\n",
    "WITH delivery_radius_events AS (\n",
    "    SELECT *\n",
    "        , DATE_TRUNC('hour', DATE_ADD(base_hour, INTERVAL '1 HOUR')) AS next_base_hour\n",
    "    FROM (\n",
    "        SELECT delivery_area_id\n",
    "            , DATE_TRUNC('hour', event_started_timestamp) AS base_hour\n",
    "\t\t\t, previous_event_started_timestamp\n",
    "            , event_started_timestamp\n",
    "            , next_event_started_timestamp\n",
    "        FROM fct.delivery_radius_events\n",
    "\t\tWHERE is_reduction\n",
    "    ) \n",
    "), delivery_radius_events_with_delta_hours AS (\n",
    "\tSELECT delivery_area_id\n",
    "\t\t, base_hour\n",
    "\t\t, next_base_hour\n",
    "\t\t, previous_event_started_timestamp\n",
    "\t\t, event_started_timestamp\n",
    "\t\t, next_event_started_timestamp\n",
    "\t    /*\n",
    "\t    When a Radius Reduction event occur, there are 4 possible scenarios:\n",
    "\t      Scenario 1. event starts & ends within the same hour\n",
    "\t      Scenario 2: event starts in the current base hour, then ends in another base hour later\n",
    "\t\t  Scenario 3: event starts in the current base hour or from a previous base hour, then ends in the current base hour\n",
    "\t      Scenario 4: event from in a previous base hour, then ends in the future base hour (i.e. full hour closure)\n",
    "\t    Where, Scenario 4 needs some special transofmration and hence won't be handled in this CTE\n",
    "\t    */\n",
    "\t\t, CASE\n",
    "\t\t\t-- Scenario 1: event starts & ends within the same hour\n",
    "\t\t\tWHEN base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\tAND base_hour = DATE_TRUNC('hour', next_event_started_timestamp)\n",
    "\t\t\t\tTHEN EXTRACT('epoch' FROM (next_event_started_timestamp - event_started_timestamp))/3600\n",
    "\t\t\t-- Scenario 2: event starts in the current base hour, then ends in another base hour later\n",
    "\t\t\tWHEN base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\tAND next_event_started_timestamp > next_base_hour\n",
    "\t\t\t\tTHEN EXTRACT('epoch' FROM (next_base_hour - event_started_timestamp))/3600\n",
    "\t\t\t-- Scenario 3: event from in a previous base hour, then ends in the current base hour\n",
    "\t\t\tWHEN previous_event_started_timestamp < base_hour\n",
    "\t\t\t\t\tAND base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\tTHEN EXTRACT('epoch' FROM (LEAST(next_event_started_timestamp, next_base_hour) - event_started_timestamp))/3600\n",
    "\t\t\tELSE 0\n",
    "\t\tEND AS delta_hours\n",
    "\tFROM delivery_radius_events\n",
    "), delivery_radius_events_with_scenarios_1_2_3 AS (\n",
    "\tSELECT delivery_area_id\n",
    "\t\t, base_hour\n",
    "\t\t, SUM(delta_hours) AS delta_hours\n",
    "\tFROM delivery_radius_events_with_delta_hours\n",
    "\tGROUP BY 1,2\n",
    "\tORDER BY delivery_area_id, base_hour\n",
    "), delivery_radius_events_with_scenario_4 AS (\n",
    "\tSELECT base.delivery_area_id\n",
    "\t\t, base.base_hour\n",
    "\t\t, 1 AS delta_hours\n",
    "\tFROM int.base_hours_with_delivery_areas base\n",
    "\tINNER JOIN delivery_radius_events_with_delta_hours events ON base.delivery_area_id = events.delivery_area_id\n",
    "\t\tAND base.base_hour BETWEEN events.event_started_timestamp AND events.next_event_started_timestamp\n",
    "\t    -- The following 2 lines are to filter double JOINs \n",
    "\t\tAND events.previous_event_started_timestamp < base.base_hour\n",
    "\t\tAND events.next_event_started_timestamp > DATE_ADD(base.base_hour, INTERVAL '1 hour')\n",
    "\tORDER BY base.delivery_area_id, base.base_hour\n",
    "), delivery_radius_events_union_all_agg AS (\n",
    "\tSELECT delivery_area_id\n",
    "\t\t, base_hour\n",
    "\t\t, SUM(delta_hours) AS delta_hours\n",
    "\tFROM (\n",
    "\t\tSELECT *\n",
    "\t\tFROM delivery_radius_events_with_scenarios_1_2_3\n",
    "\n",
    "\t\tUNION ALL\n",
    "\n",
    "\t\tSELECT *\n",
    "\t\tFROM delivery_radius_events_with_scenario_4\n",
    "\t)\n",
    "\tGROUP BY 1,2\n",
    "\tORDER BY delivery_area_id, base_hour\n",
    ")\n",
    "SELECT base.delivery_area_id\n",
    "\t, base.base_hour\n",
    "\t, COALESCE(agg.delta_hours, 0) AS delta_hours_radius_reduction\n",
    "FROM int.base_hours_with_delivery_areas base\n",
    "LEFT JOIN delivery_radius_events_union_all_agg agg ON base.base_hour = agg.base_hour\n",
    "\tAND base.delivery_area_id = agg.delivery_area_id\n",
    "ORDER BY base.delivery_area_id, base.base_hour\n",
    "'''\n",
    "\n",
    "df_sum_delivery_radius_reduction = pd.read_sql(sql_sum_delivery_radius_reduction, con = db)\n",
    "\n",
    "df_sum_delivery_radius_reduction.to_sql(\n",
    "    name = 'delivery_radius_reduction',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_sum_delivery_radius_reduction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff111975-e37d-4022-9c7c-4582b36082ac",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Delivery Radius Modifciation events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a78bcbd-162e-4b34-bb30-b43eb07b96a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_hour</th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>nb_delivery_radius_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-01 12:00:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-02 13:00:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-02 14:00:00</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-03 17:00:00</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-05 14:00:00</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            base_hour          delivery_area_id  nb_delivery_radius_modified\n",
       "0 2021-12-01 12:00:00  5cc1b60b034adf90cd8f14dd                            2\n",
       "1 2021-12-02 13:00:00  5cc1b60b034adf90cd8f14dd                            2\n",
       "2 2021-12-02 14:00:00  5d78a7e552dfabd5251dab7b                            2\n",
       "3 2021-12-03 17:00:00  5db02e5d401d690c836b9ead                            3\n",
       "4 2021-12-05 14:00:00  5db02e5d401d690c836b9ead                            2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_delivery_radius_modifications = '''\n",
    "SELECT DATE_TRUNC('hour', event_started_timestamp) AS base_hour\n",
    "\t, delivery_area_id\n",
    "\t, COUNT(*) AS nb_delivery_radius_modified\n",
    "FROM fct.delivery_radius_events\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2\n",
    "'''\n",
    "\n",
    "df_delivery_radius_modifications = pd.read_sql(sql_delivery_radius_modifications, con = db)\n",
    "df_delivery_radius_modifications.to_sql(\n",
    "    name = 'delivery_radius_modifications',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_delivery_radius_modifications.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d44970-3f6b-4c69-9fa0-0c5a0f13b2ba",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Assumption: For the WoW (Week-over-week) comparison, it is assumed to compare for the same Delivery Area, same hour in the day, and the same day of week. E.g. the number of Purchases from this Monday 1800 - 1900 would only be compared to the number of Purchases from last Monday 1800 - 1900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc55a60d-f59b-4d17-9fa0-e8652c6f5917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_hour</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>nb_purchases</th>\n",
       "      <th>end_amount_with_vat_eur</th>\n",
       "      <th>delta_hours_radius_reduction</th>\n",
       "      <th>nb_delivery_radius_modified</th>\n",
       "      <th>nb_purchases_last_week</th>\n",
       "      <th>end_amount_with_vat_eur_last_week</th>\n",
       "      <th>delta_hours_radius_reduction_last_week</th>\n",
       "      <th>nb_delivery_radius_modified_last_week</th>\n",
       "      <th>nb_purchases_wow_perc</th>\n",
       "      <th>end_amount_with_vat_eur_wow_perc</th>\n",
       "      <th>delta_hours_radius_reduction_wow_perc</th>\n",
       "      <th>nb_delivery_radius_modified_wow_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            base_hour  hour_of_day  day_of_week          delivery_area_id  \\\n",
       "0 2022-01-01 00:00:00          0.0          6.0  5cc1b60b034adf90cd8f14dd   \n",
       "1 2022-01-01 00:00:00          0.0          6.0  5d78a7e552dfabd5251dab7b   \n",
       "2 2022-01-01 00:00:00          0.0          6.0  5db02e5d401d690c836b9ead   \n",
       "3 2022-01-01 01:00:00          1.0          6.0  5cc1b60b034adf90cd8f14dd   \n",
       "4 2022-01-01 01:00:00          1.0          6.0  5d78a7e552dfabd5251dab7b   \n",
       "\n",
       "   nb_purchases  end_amount_with_vat_eur  delta_hours_radius_reduction  \\\n",
       "0             0                      0.0                           0.0   \n",
       "1             0                      0.0                           0.0   \n",
       "2             0                      0.0                           0.0   \n",
       "3             0                      0.0                           0.0   \n",
       "4             0                      0.0                           0.0   \n",
       "\n",
       "   nb_delivery_radius_modified  nb_purchases_last_week  \\\n",
       "0                            0                     NaN   \n",
       "1                            0                     NaN   \n",
       "2                            0                     NaN   \n",
       "3                            0                     NaN   \n",
       "4                            0                     NaN   \n",
       "\n",
       "   end_amount_with_vat_eur_last_week  delta_hours_radius_reduction_last_week  \\\n",
       "0                                NaN                                     NaN   \n",
       "1                                NaN                                     NaN   \n",
       "2                                NaN                                     NaN   \n",
       "3                                NaN                                     NaN   \n",
       "4                                NaN                                     NaN   \n",
       "\n",
       "   nb_delivery_radius_modified_last_week  nb_purchases_wow_perc  \\\n",
       "0                                    NaN                    0.0   \n",
       "1                                    NaN                    0.0   \n",
       "2                                    NaN                    0.0   \n",
       "3                                    NaN                    0.0   \n",
       "4                                    NaN                    0.0   \n",
       "\n",
       "   end_amount_with_vat_eur_wow_perc  delta_hours_radius_reduction_wow_perc  \\\n",
       "0                               0.0                                    0.0   \n",
       "1                               0.0                                    0.0   \n",
       "2                               0.0                                    0.0   \n",
       "3                               0.0                                    0.0   \n",
       "4                               0.0                                    0.0   \n",
       "\n",
       "   nb_delivery_radius_modified_wow_perc  \n",
       "0                                   0.0  \n",
       "1                                   0.0  \n",
       "2                                   0.0  \n",
       "3                                   0.0  \n",
       "4                                   0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_sum_purchases_delivery_radius_reduction = '''\n",
    "WITH base_hours_with_metrics AS (\n",
    "\tSELECT base.base_hour\n",
    "\t\t, EXTRACT('hour' FROM base.base_hour) AS hour_of_day\n",
    "\t\t, EXTRACT('dow' FROM base.base_hour) AS day_of_week\n",
    "\t\t, base.delivery_area_id\n",
    "\t\t, COALESCE(sum_purchases.nb_purchases, 0) AS nb_purchases\n",
    "\t\t, COALESCE(sum_purchases.end_amount_with_vat_eur, 0) AS end_amount_with_vat_eur\n",
    "\t\t, COALESCE(delivery_radius_reduction.delta_hours_radius_reduction, 0) AS delta_hours_radius_reduction\n",
    "\t\t, COALESCE(delivery_radius_modifications.nb_delivery_radius_modified, 0) AS nb_delivery_radius_modified\n",
    "\tFROM int.base_hours_with_delivery_areas base\n",
    "\tLEFT JOIN int.sum_purchases ON base.base_hour = sum_purchases.base_hour\n",
    "\t\tAND base.delivery_area_id = sum_purchases.delivery_area_id\n",
    "\tLEFT JOIN int.delivery_radius_reduction ON base.base_hour = delivery_radius_reduction.base_hour\n",
    "\t\tAND base.delivery_area_id = delivery_radius_reduction.delivery_area_id\n",
    "\tLEFT JOIN int.delivery_radius_modifications ON base.base_hour = delivery_radius_modifications.base_hour\n",
    "\t\tAND base.delivery_area_id = delivery_radius_modifications.delivery_area_id\n",
    "\tORDER BY base.base_hour, base.delivery_area_id\n",
    "), base_hours_with_metrics_and_metrics_from_last_week AS (\n",
    "\tSELECT *\n",
    "\t\t, LAG(nb_purchases) OVER (PARTITION BY delivery_area_id, hour_of_day, day_of_week ORDER BY base_hour) AS nb_purchases_last_week\n",
    "\t\t, LAG(end_amount_with_vat_eur) OVER (delivery_area_dow_hour_window) AS end_amount_with_vat_eur_last_week\n",
    "\t\t, LAG(delta_hours_radius_reduction) OVER (delivery_area_dow_hour_window) AS delta_hours_radius_reduction_last_week\n",
    "\t\t, LAG(nb_delivery_radius_modified) OVER (delivery_area_dow_hour_window) AS nb_delivery_radius_modified_last_week\n",
    "\tFROM base_hours_with_metrics\n",
    "\tWINDOW delivery_area_dow_hour_window AS (\n",
    "\t\tPARTITION BY delivery_area_id, hour_of_day, day_of_week \n",
    "\t\tORDER BY base_hour\n",
    "\t)\n",
    "\tORDER BY base_hour, delivery_area_id\n",
    ")\n",
    "SELECT *\n",
    "  -- To avoid Division by Zero error. Once again, if it is on BigQuery/Snowflake, we could just use Safe Divide functions like `DIV0`\n",
    "    , CASE \n",
    "        WHEN nb_purchases_last_week != 0 \n",
    "                AND nb_purchases_last_week IS NOT NULL\n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (nb_purchases::FLOAT - nb_purchases_last_week)/ nb_purchases_last_week\n",
    "        ELSE 0 \n",
    "      END AS nb_purchases_wow_perc\n",
    "    , CASE \n",
    "        WHEN end_amount_with_vat_eur_last_week != 0 \n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (end_amount_with_vat_eur::FLOAT - end_amount_with_vat_eur_last_week)/ end_amount_with_vat_eur_last_week\n",
    "        ELSE 0 \n",
    "      END AS end_amount_with_vat_eur_wow_perc\n",
    "    , CASE \n",
    "        WHEN delta_hours_radius_reduction_last_week != 0 \n",
    "                AND delta_hours_radius_reduction_last_week IS NOT NULL\n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (delta_hours_radius_reduction::FLOAT - delta_hours_radius_reduction_last_week)/ delta_hours_radius_reduction_last_week\n",
    "        ELSE 0 \n",
    "      END AS delta_hours_radius_reduction_wow_perc\n",
    "    , CASE \n",
    "        WHEN nb_delivery_radius_modified_last_week != 0\n",
    "                AND nb_delivery_radius_modified_last_week IS NOT NULL\n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (nb_delivery_radius_modified::FLOAT - nb_delivery_radius_modified_last_week)/ nb_delivery_radius_modified_last_week\n",
    "        ELSE 0 \n",
    "      END AS nb_delivery_radius_modified_wow_perc\n",
    "FROM base_hours_with_metrics_and_metrics_from_last_week\n",
    "'''\n",
    "\n",
    "df_sum_purchases_delivery_radius_reduction = pd.read_sql(sql_sum_purchases_delivery_radius_reduction, con = db)\n",
    "df_sum_purchases_delivery_radius_reduction.to_sql(\n",
    "    name = 'purchases_delivery_radius_reduction',\n",
    "    schema = 'sum',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_sum_purchases_delivery_radius_reduction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b5efe-005d-41f5-9403-dcb2686b62ab",
   "metadata": {},
   "source": [
    "# Solution Clarifications\n",
    "\n",
    "Thus, in addition to solving the tasks, please also answer the following questions to clarify your solution:\n",
    "* What assumptions about the data have you made to produce the dataset?\n",
    "* Why did you decide to go with this particular approach and what could be the pros and cons of applying it?\n",
    "* How could the solution be improved if given more time and data?\n",
    "* What strategy would you use for updating the dataset from task 2? Consider how often the default radius should be calculated, do we need to truncate the table before updating etc. Please assume that upstream data (i.e. purchases & delivery_radius_log) is streamed to the tables, so changes arrive near real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f92e63-e647-4bec-8503-e6798b79ad73",
   "metadata": {},
   "source": [
    "## What assumptions about the data have you made to produce the dataset?\n",
    "\n",
    "* It is assumed that the organization is adopting Dimensional Modelling (i.e. Kimball model) for data modeling practice (i.e. instead of Data Mesh, Inmon model, or Data Vault etc.).\n",
    "* It is also assumed that the option of requesting to add the primary key per event (i.e. `event_id`) is off the table for now, and we as the data team have to come up with the Surrogate Key ourselves instead of requesting the upstream Tech team to enrich it.\n",
    "  * This assumption is made due to time consideration. Once the business logic is validated and it is okay, it could be treated as a Tech Debt and move upstream in the future's refactoring effort.\n",
    "\n",
    "\n",
    "\n",
    "For further code-specific assumptions, please see the above inline comments in the code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638bca2-31b2-4141-98f0-a868f6993e52",
   "metadata": {},
   "source": [
    "## Why did you decide to go with this particular approach and what could be the pros and cons of applying it?\n",
    "\n",
    "As mentioned at the very beginning of this notebook, different layers of data are being introduced as a simple Data Warehouse architecture:\n",
    "1. `ods` schema as the Operational Data Store, the very first landing area of the ingested data (e.g. flat CSV files)\n",
    "2. `int` schema as the temp storage of the intermediate transformed data\n",
    "3. `dim`, `fct`, and `sum` schemas for different types of Data Marts (i.e. Dimension, Fact, and Summary)\n",
    "\n",
    "A couple of rationale of this architecture:\n",
    "* The data from the 2 flat files needs to be cleaned at the 1st step of entering the Data Warehouse; AND,\n",
    "* The cleaned data needs to be transformed and the same transformation should only happen once (i.e. NO duplicated code or data)\n",
    "* Having a Data Mart type-specific schema so enforce the modeling thinking while designing & consuming from the tables\n",
    "\n",
    "### Pros\n",
    "The pros of doing so mostly reaping the benefits of Dimensional Modelling as the most popular data modeling practice in the market:\n",
    "1. The easiest to understand (by business stakeholders); AND,\n",
    "2. Rather flexible to adapt to unexpected business development (e.g. introduction of new Entities, new business models, etc.);\n",
    "    * For comparison, the Inmon model for example would take a lot more planning ahead of implementation and may become a luxury for many companies with the given business dynamic in the modern days.\n",
    "3. Also, having different layers of data is essential to avoid spaghetti code - which is vital for long-term maintenance of the code\n",
    "\n",
    "Nevertheless, when the organization scales up, solely by Dimension modeling itself won't be enough to get the job done properly - it may need additional enhancement from Data Mesh (and, depending on the industry & business needs, certain elements from Data Vault should be borrowed as well). And to further enhance the implementation of Data Mesh, Data Contracts could also be considered as well.\n",
    "\n",
    "### Cons\n",
    "* Building the Data Warehouse structure requires extra investment of time & effort in short term\n",
    "* The Dimensional Modelling requires both the data team and stakeholders to have a good understanding of the Primary Key. Or else, it may lead to query performance issues or inaccurately calculated KPIs\n",
    "* Stakeholders may need further education & promotion to know how to properly use tables in `sum` schema, e.g. provided example queries\n",
    "* Changes in source data themselves are not tracked by default (only the latest state of the data will be kept). That is rather the strong suit of Data Vault, not the Kimball model's\n",
    "\n",
    "\n",
    "As for the Tech Debt part (especially the query performance-related engineering decisions), it is a calculated risk to take. When it comes to analytics, the speed of delivery is also vital - especially for time-sensitive requests. Once it is delivered, a disciplined BI/data team should always spare a certain portion of capacity (e.g. 5%) for incremental refactoring to ensure the quality of code in the long run.\n",
    "\n",
    "For further code-specific details, please see above inline comments in the code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c43cd-9b76-423d-bd4a-aee4d719ca36",
   "metadata": {},
   "source": [
    "## How could the solution be improved if given more time and data?\n",
    "\n",
    "One of the biggest constraints of the current solution is the database (the SQL dialect - to be very specific). The currently selected solution of the database is PostgreSQL v16, and the only reason for selecting it is the nature of open-source and free. \n",
    "\n",
    "Given the current size of the data (i.e. the 2 CSV  files), it is okay-ish to handle the data transformation with the above approach (e.g. using JOINs instead of Windows Function, processing the data without the required Primary Key and the `updated_at` timestamp, etc.). When the volume of the data increases as the business grows, the performance of the above queries in PostgreSQL is likely to degrade, and cost could also increase faster than expected.\n",
    "\n",
    "In addition, the usability of the produced datasets could be further enhanced for more use cases beyond the requirement of the assignment itself, e.g. \n",
    "* Include TIMESTAMPS & DATEs in the local timezone as well as the timezone topic is traditionally fairly important to the operation of this business (since the provided TIMESTAMPs are all in UTC)\n",
    "* Enrich the dataset with more Dimensions, e.g. \"T-Dimensions\" (ISO week, Month, Quarter, Year, etc.), segmentation of the Venue & the Customers, etc.\n",
    "* Collect more business use cases and enrich the same dataset with more metrics, e.g. Delivery Time (time interval between Order Time Received & Time Delivered), Distance calculation (on top of the Straight Line method, there are also other mathematical ways to calculate the distances, e.g. [Euclidean distance](https://simple.wikipedia.org/wiki/Euclidean_distance)) --> this could be more fitted to new business use cases, for example improving delivery performance in new countries that has very different urban design\n",
    "\n",
    "In addition, the data still looks like more \"downstream\" in the picture. It may be an indicator of the team mentality (both data team and tech teams) - seeing data as a by-product of the applications/business or seeing data itself as a Product. If the awareness of seeing data as a Product is in place, the Primary Key of the events should be directly available in the data source instead of having to generate the surrogate key downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a97cc-d17d-4e08-8fa3-f0fc2b3cda09",
   "metadata": {},
   "source": [
    "## What strategy would you use for updating the dataset from task 2?  Consider how often the default radius should be calculated, do we need to truncate the table before updating etc. Please assume that upstream data (i.e. purchases & delivery_radius_log) is streamed to the tables, so changes arrive near real time.\n",
    "\n",
    "This is a very good question, and it deserves a detail explanation here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4680257-ceb5-474c-a73c-a53693f64f2b",
   "metadata": {},
   "source": [
    "Essentially, the nature of the Task 2 dataset is a Summary table containing pre-computed & pre-aggregated Metrics from both Sales (Purchases & Revenue) and Delivery Radius Events (Radius Reduction & Radius Modifications).\n",
    "\n",
    "### Elements to be involved \n",
    "\n",
    "#### The combined key of the Summary table\n",
    "In the summary table sum.purchases_delivery_radius_reduction, the columns being used as combine key are:\n",
    "\n",
    "* `base_hour`\n",
    "* `delivery_area_id`\n",
    "\n",
    "These would be very vital when it comes to refreshing the table.\n",
    "\n",
    "#### Partitioning & Clustering\n",
    "Before actually implementing the dataset, there are MUST-HAVEs to be put in place for the sake of query performance & cost:\n",
    "* Partitioning (DATE/TIMESTAMP Partitioning in BigQuery, Micro-Partitions in Snowflake)\n",
    "* Clustering\n",
    "\n",
    "These 2 elements are extremely important when it comes to high-volume & high-velocity data (e.g. tracking vents, or the `delivery_radius_log` in this assignment). Proper Partitioning would help the data warehouse skip irrelevant rows of records which leads to good performance, Clustering would help speed up identifying the relevant records to process. \n",
    "\n",
    "### Factors to consider\n",
    "There are a couple of factors that should be considered regarding deciding the Data Updating Strategy:\n",
    "* Freshness of the upstream data (i.e. `purchases` & `delivery_radius_log`) - and it is known to be near real-time\n",
    "* Business Requirement: Even if it is technically possible, it doesn't mean that it **has to** be delivered on near-realtime as well. At the end of the day, it depends on the business use-case.\n",
    "  * For example, if it is just used for daily reporting/dashboarding, then just a daily refresh would be enough;\n",
    "  * On the other hand, if someone from the business team is actively looking at those numbers literally a few times every hour AND makes decisions based on it, then **maybe** it makes sense to refresh on an hourly basis.\n",
    "    * This sort of business use case needs a more proper tech solution instead of depending on an analytical dataset, with the rationale of the resources required to maintain the SLA and the stability of the pipeline since high-velocity data pipelines tend to be more volatile than slower ones.\n",
    "* The Business Definitions of the metrics:\n",
    "  * The Sales related Metrics (Purchases & Revenue) are rather straightforward, as they often just anchor on the \"base TIMESTAMP\" (e.g. `time_received`) unless there is a special requirement from the business logic\n",
    "  * Yet, the trick lies with the event-based data interval calculation - namely Delivery Radius Reduction. It is tricky as the given definition of the Default Delivery Radius solely depends on the Delivery Radius sticking around for more than 24 hours.\n",
    "    * A better scenario would be whatever application or micro-service generating the change of Default Radius events available somewhere else (e.g. another Kafka topic) instead of populating everything in just 1 Kafka topic and ask the Data/BI team to do the transformation.\n",
    "      * The rationale behind is the principle of \"Shift Left\" - doing so at the beginning would just cost 1 Euro, doing so in the middle would cost 10 Euro, and doing it at the end could cost 100+ Euro.\n",
    "\n",
    "\n",
    "#### Proposed Strategy\n",
    "Based on the factors mentioned above, here would be the spec of the Proposed update strategy:\n",
    "* Materialization: Incremental\n",
    "  * The TIMESTAMPs from the upstream tables (`purchases` & `delivery_radius_log`) would be required here to implement incremental filtering\n",
    "    * it is also equally important to plan for backfills in the future. Ideally speaking, there should be a condition statement (i.e. `IF`) to check if the current run is an Incremental one. If not, then the TIMESTAMP filter should be removed from the SQL statement to allow the data warehouse to process the whole table\n",
    "  * The combined key of `base_hour` + `delivery_area_id` in the Summary table would leverage to identify which exact record contains new Metrics from the run and hence should be updated as well\n",
    "* Interval: Hourly/Daily (depends on the business use case)\n",
    "  * It is mostly based on the required Dimension being on an hourly basis. Although it is technically possible to make it even more frequent, the cost of the additional volatility and potential confusion for the stakeholders (e.g. very low numbers of the latest record as the hour hasn't been completed yet)\n",
    "* Regular backfills & Re-clustering (e.g. every quarter/half-year, or even on an annual basis)\n",
    "  * While the Incremental load is good for performance & cost, if there are anomalies of the upstream events (e.g. the events arrive late but with a much older TIMESTAMP value), those records will never be covered by the incremental load\n",
    "  * While a table has records being inserted only for a certain period, the clustering would slowly be messy over time. It might be beneficial to have it re-clustered once a while \n",
    "\n",
    "***Notes***\n",
    "* It is not recommended that the pattern of truncating the table first due to resilience consideration.\n",
    "  * Depending on which data warehouse is being used (e.g. Snowflake), there is a chance truncating operations would cause problems. Since truncating and inserting are different queries, if there is a service disruption and somehow makes one of the queries fail, it would result in duplication or unexpected missing records (there is also a risk of having the whole table empty if human error is in the mix)\n",
    "* Last but not least, the use of Incremental Load is a 2-edged sword. It brings the benefit of performance & cost, while it also introduces extra complexity. It is vital to ensure the foundation & tech debt are in a good state to implement this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d7144-16f9-4558-bf15-fb6c403df7ba",
   "metadata": {},
   "source": [
    "#### Additional considerations\n",
    "\n",
    "##### Missing columns from the given `delivery_radius_log`\n",
    "\n",
    "As a starter, there are 2 very critical fields/columns missing from the `delivery_radius_log`:\n",
    "1. A Primary Key (e.g. `event_id`)\n",
    "    * A Surrogate Key can still be derived by using both of `delivery_area_id` & `event_started_timestamp`. It is okay in most cases, **but** in certain corner cases (e.g. duplicated events from the upstream application/micro-service, Kafka events replay), even the Surrogate Key would be duplicated.\n",
    "3. TIMESTAMPs indicating when is the event being ingested into the data warehouse (e.g. `created_at`, `updated_at`, or `ingested_at`)\n",
    "\n",
    "Imagine if these 2 columns exist in `delivery_radius_log`, the schema would look like this:\n",
    "* `event_id`\n",
    "* `delivery_area_id`\n",
    "* `delivery_radius_meters`\n",
    "* `event_started_timestamp`\n",
    "* `ingested_at`\n",
    "\n",
    "then, it would be possible to leverage the strength of the data warehouse (e.g. Snowflake, BigQuery) to do something like this:\n",
    "\n",
    "```SQL\n",
    "SELECT * EXCEPT (_row_number)\n",
    "FROM (\n",
    "    SELECT *\n",
    "        , ROW_NUMBER() OVER (PARTITION BY event_id ORDER BY ingested_at DESC) AS _row_number\n",
    "    FROM delivery_radius_log\n",
    ")\n",
    "WHERE _row_number = 1\n",
    "```\n",
    "\n",
    "If this is in place, the data pipeline would still be fairly resilient even if there are duplications from the streaming events (e.g. replaying in Kafka)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
