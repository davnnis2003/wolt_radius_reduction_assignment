{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29593d68-667b-4aae-8394-0df37dabf9fd",
   "metadata": {},
   "source": [
    "# Overview of this Notebook and the approach\n",
    "\n",
    "## Toolings used\n",
    "* Python v3.11\n",
    "  * Pandas\n",
    "  * psycopg2\n",
    "  * sqlalchemy\n",
    "* Jupyter Notebook\n",
    "* PostgreSQL v16\n",
    "\n",
    "\n",
    "## The schema of the simple DWH (Data Warehouse) in PostgreSQL\n",
    "* `ods`: It stands for Operational Data Store, the very first landing area of the flat CSV files in the DWH\n",
    "* `int`: An intermediate layer for storing tables during the transformation process\n",
    "* `dim`: A delicate schema for Dimension tables. See [Dimensional modeling](https://en.wikipedia.org/wiki/Dimensional_modeling)\n",
    "* `fct`: A delicate schema for Fact tables.\n",
    "* `sum`: A delicate schema for Summary tables which contain pre-computed & pre-aggregated business metrics, consuming from tables in the `dim` & `fct` schemas only.\n",
    "\n",
    "### Data Ingestion\n",
    "It was done with Python ( Pandas, psycopg2, and sqlalchemy) and ingesting the 2 given CSV files (i.e. `purchases` & `delivery_radius_log`) into the `ods` schema in the PostgreSQL DWH.\n",
    "\n",
    "### Data Transformation\n",
    "The goal of the Data Transformation is usually creating tables (Data Marts) in the `dim`, `fct`, and `sum` schemas.\n",
    "\n",
    "For simple Data Marts (e.g. the Dimension Table of `delivery_areas`), it would directly consume from `ods`. If the transformation is complex, `int` is used to store the intermediate transformed tables.\n",
    "\n",
    "In addition, if there is a lot more data than the given task here, potentially an extra layer between the `ods`  and the `int` schema can be added to centralize all the cleaning logic. It can be named as [the \"Staging layer\"](https://medium.com/data-panda/dbt-models-staging-layer-55f0f2ddc5e4) like dbt Labs, or just the \"Data Layer\"/\"Merge Layer\" from a more old school practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea84d85-e39d-47ae-8eb3-ae8276b08d97",
   "metadata": {},
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d25597f-b712-483a-b3d9-4d0439c59e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all packages used\n",
    "import pandas as pd\n",
    "import psycopg2 \n",
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6c4f0-701a-478d-a9c9-cb5a15bc143a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting up a PostgreSQL DB  in localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67897c2-46e5-474b-84a1-a15e00ef2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "postgresql+psycopg2://dbuser:***@localhost/postgres"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a connection STRING for using a test service account `dbuser`\n",
    "from sqlalchemy import URL\n",
    "\n",
    "url_object = URL.create(\n",
    "    \"postgresql+psycopg2\",\n",
    "    username=\"dbuser\",\n",
    "    password=\"1\",  # plain (unescaped) text\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    ")\n",
    "\n",
    "url_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55e64283-0dc6-4647-a5bd-ad6e0a9d1ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test SELECT statement...\n",
      "\n",
      "result:\n",
      "\n",
      "[(1,)]\n"
     ]
    }
   ],
   "source": [
    "# Establish the connection to the PostgreSQL instance, and create a test table there to confirm the connection works as expected\n",
    "db = create_engine(url_object) \n",
    "conn = db.connect() \n",
    "conn1 = psycopg2.connect( \n",
    "  database=\"postgres\", \n",
    "  user='dbuser',  \n",
    "  password='1',  \n",
    "  host='localhost',  \n",
    "  port= '5432'\n",
    ") \n",
    "  \n",
    "conn1.autocommit = True\n",
    "cursor = conn1.cursor() \n",
    "  \n",
    "# drop table if it already exists \n",
    "cursor.execute('drop table if exists ods.test_table') \n",
    "  \n",
    "sql = '''\n",
    "CREATE TABLE ods.test_table AS \n",
    "SELECT 1 AS test_column_name\n",
    ";\n",
    "'''\n",
    "  \n",
    "cursor.execute(sql) \n",
    "\n",
    "sql_test = '''\n",
    "SELECT *\n",
    "FROM ods.test_table\n",
    ";\n",
    "'''\n",
    "\n",
    "print('running test SELECT statement...\\n')\n",
    "cursor.execute(sql_test) \n",
    "results = cursor.fetchall() \n",
    "print('result:\\n')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "177e800e-7d78-43e9-be9c-45ecc7f708f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 1 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   test_column_name  1 non-null      int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 140.0 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6h/ptt_z0_53nb5t746v0n1sv980000gr/T/ipykernel_47276/1069515249.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_check = pd.read_sql_query(sql_test, conn1)\n"
     ]
    }
   ],
   "source": [
    "df_check = pd.read_sql_query(sql_test, conn1)\n",
    "df_check.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "71f19486-3c95-42f9-ae2f-6bdf1bdc5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the test table\n",
    "cursor.execute('drop table ods.test_table') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ed014d-f967-4f52-9462-7484fb5bfc9c",
   "metadata": {},
   "source": [
    "## Creating ODS tables in the local PostgreSQL server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d4872-70f6-4778-bb03-21f83040dc6d",
   "metadata": {},
   "source": [
    "### Importing delivery_radius_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ef486c-ee77-4b03-89fc-a6a72b76c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1316 entries, 0 to 1315\n",
      "Data columns (total 3 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   DELIVERY_AREA_ID         1316 non-null   object\n",
      " 1   DELIVERY_RADIUS_METERS   1316 non-null   int64 \n",
      " 2   EVENT_STARTED_TIMESTAMP  1316 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 31.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Importing the CSV file delivery_radius_log as a dataframe first\n",
    "df_delivery_radius_log = pd.read_csv('data/delivery_radius_log.csv')\n",
    "df_delivery_radius_log.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6051f81-8784-40f7-9a65-6e263b44ea51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DELIVERY_AREA_ID</th>\n",
       "      <th>DELIVERY_RADIUS_METERS</th>\n",
       "      <th>EVENT_STARTED_TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>3000</td>\n",
       "      <td>2022-06-14T08:26:20.923854Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>7000</td>\n",
       "      <td>2022-06-14T08:49:01.186365Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>3000</td>\n",
       "      <td>2022-06-18T07:43:57.662294Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>7000</td>\n",
       "      <td>2022-06-18T08:00:45.227506Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>4000</td>\n",
       "      <td>2022-06-18T08:05:29.093983Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DELIVERY_AREA_ID  DELIVERY_RADIUS_METERS  \\\n",
       "0  5db02e5d401d690c836b9ead                    3000   \n",
       "1  5db02e5d401d690c836b9ead                    7000   \n",
       "2  5db02e5d401d690c836b9ead                    3000   \n",
       "3  5db02e5d401d690c836b9ead                    7000   \n",
       "4  5d78a7e552dfabd5251dab7b                    4000   \n",
       "\n",
       "       EVENT_STARTED_TIMESTAMP  \n",
       "0  2022-06-14T08:26:20.923854Z  \n",
       "1  2022-06-14T08:49:01.186365Z  \n",
       "2  2022-06-18T07:43:57.662294Z  \n",
       "3  2022-06-18T08:00:45.227506Z  \n",
       "4  2022-06-18T08:05:29.093983Z  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exmaining the DataFrame\n",
    "df_delivery_radius_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5a738f36-e501-4cb3-a8c7-3d5c8f1c2107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting the file into the PostgreSQL DB instance after checking it is okay\n",
    "df_delivery_radius_log.to_sql(\n",
    "    name = 'delivery_radius_log',\n",
    "    schema = 'ods',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a3b81-1e15-4902-9467-0b302f6e9ba5",
   "metadata": {},
   "source": [
    "### Importing purchases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d280eef0-22b9-4b71-8d8d-49d4daa700db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 177895 entries, 0 to 177894\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   PURCHASE_ID                            177895 non-null  object \n",
      " 1   TIME_RECEIVED                          177895 non-null  object \n",
      " 2   TIME_DELIVERED                         177895 non-null  object \n",
      " 3   END_AMOUNT_WITH_VAT_EUR                177895 non-null  float64\n",
      " 4   DROPOFF_DISTANCE_STRAIGHT_LINE_METRES  177895 non-null  int64  \n",
      " 5   DELIVERY_AREA_ID                       177895 non-null  object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Importing the CSV file delivery_radius_log as a dataframe first\n",
    "df_purchases = pd.read_csv('data/purchases.csv')\n",
    "df_purchases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a6fe16-c2ef-411b-b02e-2110bf2173b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PURCHASE_ID</th>\n",
       "      <th>TIME_RECEIVED</th>\n",
       "      <th>TIME_DELIVERED</th>\n",
       "      <th>END_AMOUNT_WITH_VAT_EUR</th>\n",
       "      <th>DROPOFF_DISTANCE_STRAIGHT_LINE_METRES</th>\n",
       "      <th>DELIVERY_AREA_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f85beff7762a1539ad6faf1</td>\n",
       "      <td>2022-10-13T14:51:43.048Z</td>\n",
       "      <td>2022-10-13T15:18:35.265Z</td>\n",
       "      <td>17.87</td>\n",
       "      <td>735</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f85c08dddf0c9826389f3cd</td>\n",
       "      <td>2022-10-13T14:58:21.078Z</td>\n",
       "      <td>2022-10-13T15:28:09.194Z</td>\n",
       "      <td>17.75</td>\n",
       "      <td>436</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f85bc2cf49ddea98955ce5f</td>\n",
       "      <td>2022-10-13T14:39:40.153Z</td>\n",
       "      <td>2022-10-13T15:05:15.058Z</td>\n",
       "      <td>25.80</td>\n",
       "      <td>867</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f855dbf5a93deaf2be5b872</td>\n",
       "      <td>2022-10-13T07:56:47.003Z</td>\n",
       "      <td>2022-10-13T09:05:14.37Z</td>\n",
       "      <td>15.70</td>\n",
       "      <td>252</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5f85be8a8876393ee141ed82</td>\n",
       "      <td>2022-10-13T14:49:46.693Z</td>\n",
       "      <td>2022-10-13T15:14:31.299Z</td>\n",
       "      <td>18.80</td>\n",
       "      <td>857</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PURCHASE_ID             TIME_RECEIVED  \\\n",
       "0  5f85beff7762a1539ad6faf1  2022-10-13T14:51:43.048Z   \n",
       "1  5f85c08dddf0c9826389f3cd  2022-10-13T14:58:21.078Z   \n",
       "2  5f85bc2cf49ddea98955ce5f  2022-10-13T14:39:40.153Z   \n",
       "3  5f855dbf5a93deaf2be5b872  2022-10-13T07:56:47.003Z   \n",
       "4  5f85be8a8876393ee141ed82  2022-10-13T14:49:46.693Z   \n",
       "\n",
       "             TIME_DELIVERED  END_AMOUNT_WITH_VAT_EUR  \\\n",
       "0  2022-10-13T15:18:35.265Z                    17.87   \n",
       "1  2022-10-13T15:28:09.194Z                    17.75   \n",
       "2  2022-10-13T15:05:15.058Z                    25.80   \n",
       "3   2022-10-13T09:05:14.37Z                    15.70   \n",
       "4  2022-10-13T15:14:31.299Z                    18.80   \n",
       "\n",
       "   DROPOFF_DISTANCE_STRAIGHT_LINE_METRES          DELIVERY_AREA_ID  \n",
       "0                                    735  5d78a7e552dfabd5251dab7b  \n",
       "1                                    436  5cc1b60b034adf90cd8f14dd  \n",
       "2                                    867  5cc1b60b034adf90cd8f14dd  \n",
       "3                                    252  5db02e5d401d690c836b9ead  \n",
       "4                                    857  5db02e5d401d690c836b9ead  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exmining the DataFrame example data\n",
    "df_purchases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3fa0027-439f-472c-ad1b-02d10c1b7b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting the DataFrame into the PostgreSQL DB instance\n",
    "df_purchases.to_sql(\n",
    "    name = 'purchases',\n",
    "    schema = 'ods',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255aac7-5ef7-4585-af89-ceb7f6929a05",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "In the first task youâ€™ll work with the delivery radius log dataset. Given this delivery radius change log, we would like you to detect at any given time what is a temporary reduction (or increase) of the delivery radius and what is the \"default\" (more permanent) delivery radius. For this exercise, you can assume that the default radius at any given time is a radius that has lasted for at least 24 hours uninterrupted.\n",
    "\n",
    "We would like you to produce a dataset(s) and answer the following:\n",
    "* What are all the default delivery radiuses for the delivery areas during the timeframe\n",
    "provided? Keep in mind that each area can have multiple default radiuses in the given\n",
    "dataset.\n",
    "* How many hours of radius reductions with respect to the the default radiuses have we\n",
    "had during the timeframe provided for each delivery area?\n",
    "\n",
    "Please give answers in numerical values to the above questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "459a6ef2-b157-4836-b2af-671fa93c875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>event_started_timestamp</th>\n",
       "      <th>previous_event_started_timestamp</th>\n",
       "      <th>next_event_started_timestamp</th>\n",
       "      <th>delta_hours</th>\n",
       "      <th>is_default_delivery_radius</th>\n",
       "      <th>delivery_radius_meters</th>\n",
       "      <th>current_default_delivery_radius_meters</th>\n",
       "      <th>is_reduction</th>\n",
       "      <th>delta_hours_radius_reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2487a170b8791b235fc1501c273bb56c</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-01 12:12:41.947087</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12b9779d17cb30c7034200cf3dbb6cb7</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>2021-12-01 12:12:41.947087</td>\n",
       "      <td>2021-12-02 13:16:21.329693</td>\n",
       "      <td>0.290961</td>\n",
       "      <td>False</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ca8ba8492a3e736147d95a4dbe25c1c</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-02 13:16:21.329693</td>\n",
       "      <td>2021-12-01 12:30:09.405860</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>24.769979</td>\n",
       "      <td>True</td>\n",
       "      <td>3500</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c7052ba8f0e2df79f0e1dabb4404b8a1</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>2021-12-02 13:16:21.329693</td>\n",
       "      <td>2021-12-05 15:52:54.673552</td>\n",
       "      <td>0.177635</td>\n",
       "      <td>False</td>\n",
       "      <td>6500</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6304e37f63d8eecdc2c63093af33c765</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2021-12-05 15:52:54.673552</td>\n",
       "      <td>2021-12-02 13:27:00.815321</td>\n",
       "      <td>2021-12-05 16:11:52.970808</td>\n",
       "      <td>74.431627</td>\n",
       "      <td>True</td>\n",
       "      <td>3500</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           event_id          delivery_area_id  \\\n",
       "0  2487a170b8791b235fc1501c273bb56c  5cc1b60b034adf90cd8f14dd   \n",
       "1  12b9779d17cb30c7034200cf3dbb6cb7  5cc1b60b034adf90cd8f14dd   \n",
       "2  3ca8ba8492a3e736147d95a4dbe25c1c  5cc1b60b034adf90cd8f14dd   \n",
       "3  c7052ba8f0e2df79f0e1dabb4404b8a1  5cc1b60b034adf90cd8f14dd   \n",
       "4  6304e37f63d8eecdc2c63093af33c765  5cc1b60b034adf90cd8f14dd   \n",
       "\n",
       "     event_started_timestamp previous_event_started_timestamp  \\\n",
       "0 2021-12-01 12:12:41.947087                              NaT   \n",
       "1 2021-12-01 12:30:09.405860       2021-12-01 12:12:41.947087   \n",
       "2 2021-12-02 13:16:21.329693       2021-12-01 12:30:09.405860   \n",
       "3 2021-12-02 13:27:00.815321       2021-12-02 13:16:21.329693   \n",
       "4 2021-12-05 15:52:54.673552       2021-12-02 13:27:00.815321   \n",
       "\n",
       "  next_event_started_timestamp  delta_hours is_default_delivery_radius  \\\n",
       "0   2021-12-01 12:30:09.405860          NaN                       None   \n",
       "1   2021-12-02 13:16:21.329693     0.290961                      False   \n",
       "2   2021-12-02 13:27:00.815321    24.769979                       True   \n",
       "3   2021-12-05 15:52:54.673552     0.177635                      False   \n",
       "4   2021-12-05 16:11:52.970808    74.431627                       True   \n",
       "\n",
       "   delivery_radius_meters  current_default_delivery_radius_meters  \\\n",
       "0                    3500                                     NaN   \n",
       "1                    6500                                     NaN   \n",
       "2                    3500                                  3500.0   \n",
       "3                    6500                                  3500.0   \n",
       "4                    3500                                  3500.0   \n",
       "\n",
       "  is_reduction  delta_hours_radius_reduction  \n",
       "0         None                           0.0  \n",
       "1         None                           0.0  \n",
       "2        False                           0.0  \n",
       "3        False                           0.0  \n",
       "4        False                           0.0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fact table in the DB for re-using\n",
    "sql_fct_delivery_areas_default_radius_events = '''\n",
    "WITH delivery_radius_log AS (\n",
    "    SELECT *\n",
    "        , LAG(delivery_radius_meters) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp) AS previous_delivery_radius_meters\n",
    "        , LAG(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp) AS previous_event_started_timestamp \n",
    "        , LEAD(event_started_timestamp) OVER (PARTITION BY delivery_area_id ORDER BY event_started_timestamp) AS next_event_started_timestamp \n",
    "    FROM (\n",
    "        -- PostgreSQL syntax issue, need to use `\"` to specify the exact column name\n",
    "        SELECT \"DELIVERY_AREA_ID\" AS delivery_area_id\n",
    "            , \"EVENT_STARTED_TIMESTAMP\"::TIMESTAMP AS event_started_timestamp\n",
    "            , \"DELIVERY_RADIUS_METERS\" AS delivery_radius_meters\n",
    "        FROM ods.delivery_radius_log \n",
    "    )\n",
    "), fct_delivery_radius_log AS (\n",
    "    SELECT *\n",
    "      , EXTRACT('epoch' FROM (event_started_timestamp - previous_event_started_timestamp))/3600 AS delta_hours\n",
    "    FROM delivery_radius_log\n",
    "), main_query AS (\n",
    "    -- Generate a surrogate key as the Primary Key of the events are missing\n",
    "\tSELECT MD5(CONCAT(delivery_area_id, event_started_timestamp::TEXT)) AS event_id \n",
    "\t\t, delivery_area_id\n",
    "\t\t, event_started_timestamp\n",
    "\t\t, previous_event_started_timestamp\n",
    "\t\t, next_event_started_timestamp\n",
    "\t\t, delta_hours\n",
    "\t\t, (delta_hours >= 24) AS is_default_delivery_radius\n",
    "\t\t, delivery_radius_meters\n",
    "\t\t, previous_delivery_radius_meters\n",
    "\tFROM fct_delivery_radius_log\n",
    "\tORDER BY delivery_area_id, event_started_timestamp\n",
    "), default_delivery_radius AS (\n",
    "    -- Workaround since PostgreSQL doesn't support IGNORE NULLS clause\n",
    "\tSELECT *\n",
    "\tFROM main_query\n",
    "\tWHERE is_default_delivery_radius\n",
    "), main_query_with_default_delivery_radius_meters AS (\n",
    "\tSELECT event_id \n",
    "\t\t, delivery_area_id\n",
    "\t\t, event_started_timestamp\n",
    "\t\t, previous_event_started_timestamp\n",
    "\t\t, next_event_started_timestamp\n",
    "\t\t, delta_hours\n",
    "\t\t, is_default_delivery_radius\n",
    "\t\t, delivery_radius_meters\n",
    "\t\t, CASE\n",
    "\t\t\tWHEN is_default_delivery_radius\n",
    "\t\t\t\tTHEN delivery_radius_meters\n",
    "\t\t\tELSE current_default_delivery_radius_meters\n",
    "\t\t  END AS current_default_delivery_radius_meters\n",
    "\tFROM (\n",
    "\t\tSELECT main_query.*\n",
    "\t\t\t, default_delivery_radius.delivery_radius_meters AS current_default_delivery_radius_meters\n",
    "\t\t\t, ROW_NUMBER() OVER (PARTITION BY main_query.event_id ORDER BY main_query.event_started_timestamp) AS _row_number\n",
    "\t\tFROM main_query\n",
    "\t\tLEFT JOIN default_delivery_radius ON main_query.delivery_area_id = default_delivery_radius.delivery_area_id\n",
    "\t\t\tAND main_query.event_started_timestamp > default_delivery_radius.event_started_timestamp\n",
    "\t)\n",
    "\tWHERE _row_number = 1\n",
    "\tORDER BY delivery_area_id, event_started_timestamp\n",
    ")\n",
    "SELECT *\n",
    "\t, (delivery_radius_meters < current_default_delivery_radius_meters) AS is_reduction\n",
    "\t, CASE\n",
    "\t\tWHEN delivery_radius_meters < current_default_delivery_radius_meters\n",
    "\t\t\tTHEN delta_hours\n",
    "\t\tELSE 0\n",
    "\t  END AS delta_hours_radius_reduction\n",
    "FROM main_query_with_default_delivery_radius_meters\n",
    "'''\n",
    "\n",
    "df_fct_delivery_areas_default_radius_events = pd.read_sql(sql_fct_delivery_areas_default_radius_events, con = db)\n",
    "\n",
    "df_fct_delivery_areas_default_radius_events.to_sql(\n",
    "    name = 'delivery_areas_default_radius_events',\n",
    "    schema = 'fct',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_fct_delivery_areas_default_radius_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bcdf75-207d-4810-bdfe-8699eef748f2",
   "metadata": {},
   "source": [
    "* The output table above is a changelog of per Delivery Areas (`delivery_area_id`) with their default Delivery Radius (i.e. `default_delivery_radius_meters`) over time. Given the dynamic nature of the Delivery Area radius, it is better to deliver a Fact table instead of a Dimension table (or Summary table).\n",
    "* A simple `SUM` on the column `delta_hours_radius_reduction` would be able to generate the hours of radius reductions with respect to the default radiuses, e.g.\n",
    "```sql\n",
    "SELECT delivery_area_id\n",
    "  , SUM(delta_hours_radius_reduction) AS delta_hours_radius_reduction\n",
    "FROM fct.delivery_areas_default_radius_events\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "```\n",
    "\n",
    "* Important Note: The current query is not perfect per se and could use some refactoring for when the dataset grows bigger in the future. If it is on proper DWH solutions (e.g. Google BigQuery, Snowflake etc.), using Windows Function with IGNORE NULLS would be a lot more scalable way to do so compared to JOINs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe280c-24d6-4017-9549-48b156082ada",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Now that we know the default delivery radiuses and times when the delivery radius was reduced, we would like you to create a derived dataset aggregated to hourly level that can be used to analyze delivery radius reductions and purchases in the areas for any hour in 2022. Build the dataset so that anyone could query the data without writing further joins or calculations and would be able to answer the following questions with a simple SELECT statement:\n",
    "* How many purchases and how much revenue (End Amount With VAT Eur) do we produce during the hour?\n",
    "* How long do the deviations (reductions) from default radius last during the hour? How many times have we modified the radius during the hour?\n",
    "* How do these hourly values compare to the previous week for each area? This is just a simple week-over-week percentage difference for each of the above-mentioned four measures.\n",
    "\n",
    "We want to emphasize that all three questions should be answered with the same aggregated dataset, meaning for instance that even the week-over-week differences are pre-calculated. Please note that for this task it is enough to only create the dataset and you are not expected to answer these questions. We only wish to see the code which creates this table and a sample of a few rows from the resulting dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013c015-f4f4-49fb-9fbe-ed35212650c4",
   "metadata": {},
   "source": [
    "## Prep work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ba871-3ac2-47dc-b48a-4180acd4a4ff",
   "metadata": {},
   "source": [
    "### Generate a Fact table of Purchases\n",
    "\n",
    "The structure of the ODS table of `purchases` isn't good enough and need some enrichment with new columns, and they will be vital for later transformation. This step is to build a proper Fact table based on it and enforce the data type per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2634a8f3-18ba-4d1a-aa90-3f6d537a478b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_id</th>\n",
       "      <th>time_received</th>\n",
       "      <th>time_delivered</th>\n",
       "      <th>end_amount_with_vat_eur</th>\n",
       "      <th>dropoff_distance_straight_line_metres</th>\n",
       "      <th>delivery_area_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e0bf2fdc3cded7e3ee098ae</td>\n",
       "      <td>2022-01-01 01:16:45.557</td>\n",
       "      <td>2022-01-02 09:36:24.341</td>\n",
       "      <td>28.90</td>\n",
       "      <td>362</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e0c53153040aacc8319ebd7</td>\n",
       "      <td>2022-01-01 08:06:45.992</td>\n",
       "      <td>2022-01-01 08:25:41.983</td>\n",
       "      <td>30.05</td>\n",
       "      <td>454</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e0c55e571bf4d9b33e0a3c0</td>\n",
       "      <td>2022-01-01 08:18:45.382</td>\n",
       "      <td>2022-01-01 08:44:36.997</td>\n",
       "      <td>18.70</td>\n",
       "      <td>1288</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e0c56163040aacc8319ef65</td>\n",
       "      <td>2022-01-01 08:19:34.633</td>\n",
       "      <td>2022-01-01 08:51:02.019</td>\n",
       "      <td>32.00</td>\n",
       "      <td>2044</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e0c577d93a422bd1b6dc02d</td>\n",
       "      <td>2022-01-01 08:25:33.776</td>\n",
       "      <td>2022-01-01 08:56:42.016</td>\n",
       "      <td>39.20</td>\n",
       "      <td>2414</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                purchase_id           time_received          time_delivered  \\\n",
       "0  5e0bf2fdc3cded7e3ee098ae 2022-01-01 01:16:45.557 2022-01-02 09:36:24.341   \n",
       "1  5e0c53153040aacc8319ebd7 2022-01-01 08:06:45.992 2022-01-01 08:25:41.983   \n",
       "2  5e0c55e571bf4d9b33e0a3c0 2022-01-01 08:18:45.382 2022-01-01 08:44:36.997   \n",
       "3  5e0c56163040aacc8319ef65 2022-01-01 08:19:34.633 2022-01-01 08:51:02.019   \n",
       "4  5e0c577d93a422bd1b6dc02d 2022-01-01 08:25:33.776 2022-01-01 08:56:42.016   \n",
       "\n",
       "   end_amount_with_vat_eur  dropoff_distance_straight_line_metres  \\\n",
       "0                    28.90                                    362   \n",
       "1                    30.05                                    454   \n",
       "2                    18.70                                   1288   \n",
       "3                    32.00                                   2044   \n",
       "4                    39.20                                   2414   \n",
       "\n",
       "           delivery_area_id  \n",
       "0  5db02e5d401d690c836b9ead  \n",
       "1  5db02e5d401d690c836b9ead  \n",
       "2  5db02e5d401d690c836b9ead  \n",
       "3  5cc1b60b034adf90cd8f14dd  \n",
       "4  5cc1b60b034adf90cd8f14dd  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_fct_purchases = '''\n",
    "SELECT \"PURCHASE_ID\" AS purchase_id\n",
    "\t, \"TIME_RECEIVED\"::TIMESTAMP AS time_received\n",
    "\t, \"TIME_DELIVERED\"::TIMESTAMP AS time_delivered\n",
    "\t, \"END_AMOUNT_WITH_VAT_EUR\" AS end_amount_with_vat_eur\n",
    "\t, \"DROPOFF_DISTANCE_STRAIGHT_LINE_METRES\" AS dropoff_distance_straight_line_metres\n",
    "\t, \"DELIVERY_AREA_ID\" AS delivery_area_id\n",
    "FROM ods.purchases\n",
    "ORDER BY time_received\n",
    "\n",
    "'''\n",
    "\n",
    "df_fct_purchases = pd.read_sql(sql_fct_purchases, con = db)\n",
    "\n",
    "df_fct_purchases.to_sql(\n",
    "    name = 'purchases',\n",
    "    schema = 'fct',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_fct_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25014a-e0db-4a88-8f6d-f598216929d1",
   "metadata": {},
   "source": [
    "### Generate a Dimension table for all Delivery Areas\n",
    "\n",
    "Although the Fact table of Default Delivery Radius is there already, it is still useful to have a Dimension table having `delivery_area_id` as the Primary Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "edd8f1bf-6fdc-43ae-a94b-2e30a214ff96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id\n",
       "0  5db02e5d401d690c836b9ead\n",
       "1  5cc1b60b034adf90cd8f14dd\n",
       "2  5d78a7e552dfabd5251dab7b"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dim_delivery_areas = '''\n",
    "SELECT DISTINCT \"DELIVERY_AREA_ID\" AS delivery_area_id\n",
    "FROM ods.delivery_radius_log \n",
    ";\n",
    "'''\n",
    "\n",
    "df_dim_delivery_areas = pd.read_sql(sql_dim_delivery_areas, con = db)\n",
    "\n",
    "df_dim_delivery_areas.to_sql(\n",
    "    name = 'delivery_areas',\n",
    "    schema = 'dim',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_dim_delivery_areas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cca4c4-f577-458d-a68a-a6eb458bc523",
   "metadata": {},
   "source": [
    "## Development of queries for the required dataset in Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c9985-7248-4b81-871a-0135df5aaed4",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Base Hours\n",
    "\n",
    "It needs to be on a hourly basis as it is the very base of the required Summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb927c4e-09ba-4557-8a06-7193cc449876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   base_hour\n",
       "0  2022-01-01 00:00:00+01:00\n",
       "1  2022-01-01 01:00:00+01:00\n",
       "2  2022-01-01 02:00:00+01:00\n",
       "3  2022-01-01 03:00:00+01:00\n",
       "4  2022-01-01 04:00:00+01:00"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_delivery_areas = '''\n",
    "SELECT GENERATE_SERIES(date '2022-01-01', date '2022-12-31', '1 hour') AS base_hour\n",
    ";\n",
    "'''\n",
    "\n",
    "df_base_hours = pd.read_sql(sql_base_hours, con = db)\n",
    "\n",
    "df_base_hours.to_sql(\n",
    "    name = 'base_hours',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_base_hours.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1034c-e46f-4116-8681-f2d5de7cb42a",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Base Hours with all Delivery Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44f0e2-45c5-4b9a-96a0-cafcc4436e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop the Summary of Sales\n",
    "sql_delivery_areas_with_delivery_areas = '''\n",
    "WITH base_hours AS (\n",
    "    SELECT generate_series(date '2022-01-01', date '2022-12-31', '1 hour') AS base_hour\n",
    "), base_hours_with_delivery_areas AS (\n",
    "\tSELECT base_hours.base_hour\n",
    "\t\t, delivery_areas.delivery_area_id\n",
    "\tFROM base_hours\n",
    "\tCROSS JOIN dim.delivery_areas\n",
    ";\n",
    "'''\n",
    "\n",
    "df_delivery_areas_with_delivery_areas = pd.read_sql(sql_delivery_areas_with_delivery_areas, con = db)\n",
    "\n",
    "df_delivery_areas_with_delivery_areas.to_sql(\n",
    "    name = 'base_hours_with_delivery_areas',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_sum_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537e9ba-2daa-47e9-9c36-b03cf42807cc",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Purchaes Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6af78b27-dc03-434b-a944-cde108d4d7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>base_hour</th>\n",
       "      <th>nb_purchases</th>\n",
       "      <th>end_amount_with_vat_eur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>104.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 09:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>129.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 10:00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>626.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 11:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>515.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 12:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>573.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id           base_hour  nb_purchases  \\\n",
       "0  5cc1b60b034adf90cd8f14dd 2022-01-01 08:00:00             3   \n",
       "1  5cc1b60b034adf90cd8f14dd 2022-01-01 09:00:00             4   \n",
       "2  5cc1b60b034adf90cd8f14dd 2022-01-01 10:00:00            22   \n",
       "3  5cc1b60b034adf90cd8f14dd 2022-01-01 11:00:00            15   \n",
       "4  5cc1b60b034adf90cd8f14dd 2022-01-01 12:00:00            19   \n",
       "\n",
       "   end_amount_with_vat_eur  \n",
       "0                   104.65  \n",
       "1                   129.30  \n",
       "2                   626.70  \n",
       "3                   515.00  \n",
       "4                   573.75  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop the Summary of Sales\n",
    "sql_sum_purchases = '''\n",
    "SELECT delivery_area_id\n",
    "    , DATE_TRUNC('HOUR', time_received) AS base_hour\n",
    "    , COUNT(*) AS nb_purchases\n",
    "    , SUM(end_amount_with_vat_eur) AS end_amount_with_vat_eur\n",
    "FROM fct.purchases\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2\n",
    ";\n",
    "'''\n",
    "\n",
    "df_sum_purchases = pd.read_sql(sql_sum_purchases, con = db)\n",
    "\n",
    "df_sum_purchases.to_sql(\n",
    "    name = 'sum_purchases',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_sum_purchases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08901f-0c56-4bb4-b8d3-32993c248fb5",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Delivery Radius Reduction summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "177ae3fb-6c08-4912-92f8-03c3a067ea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>base_hour</th>\n",
       "      <th>delta_hours_radius_reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 00:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 01:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 02:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 03:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2022-01-01 04:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           delivery_area_id                  base_hour  \\\n",
       "0  5cc1b60b034adf90cd8f14dd  2022-01-01 00:00:00+01:00   \n",
       "1  5cc1b60b034adf90cd8f14dd  2022-01-01 01:00:00+01:00   \n",
       "2  5cc1b60b034adf90cd8f14dd  2022-01-01 02:00:00+01:00   \n",
       "3  5cc1b60b034adf90cd8f14dd  2022-01-01 03:00:00+01:00   \n",
       "4  5cc1b60b034adf90cd8f14dd  2022-01-01 04:00:00+01:00   \n",
       "\n",
       "   delta_hours_radius_reduction  \n",
       "0                           0.0  \n",
       "1                           0.0  \n",
       "2                           0.0  \n",
       "3                           0.0  \n",
       "4                           0.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop a query for the summary of radius reduction hours & # times the radius being modified\n",
    "sql_sum_delivery_radius_reduction = '''\n",
    "WITH base_hours AS (\n",
    "    SELECT generate_series(date '2022-01-01', date '2022-12-31', '1 hour') AS base_hour\n",
    "), base_hours_with_delivery_areas AS (\n",
    "\tSELECT base_hours.base_hour\n",
    "\t\t, delivery_areas.delivery_area_id\n",
    "\tFROM base_hours\n",
    "\tCROSS JOIN dim.delivery_areas\n",
    "), delivery_radius_events AS (\n",
    "    SELECT *\n",
    "        , DATE_TRUNC('hour', DATE_ADD(base_hour, INTERVAL '1 HOUR')) AS next_base_hour\n",
    "    FROM (\n",
    "        SELECT event_id\n",
    "\t\t\t, delivery_area_id\n",
    "            , DATE_TRUNC('hour', event_started_timestamp) AS base_hour\n",
    "\t\t\t, previous_event_started_timestamp\n",
    "            , event_started_timestamp\n",
    "            , next_event_started_timestamp\n",
    "            , delta_hours\n",
    "        FROM fct.delivery_areas_default_radius_events\n",
    "\t\tWHERE is_reduction\n",
    "    ) \n",
    "), delivery_radius_events_processed AS (\n",
    "    SELECT event_id\n",
    "\t\t, delivery_area_id\n",
    "\t\t, base_hour\n",
    "\t\t, next_base_hour\n",
    "\t\t, previous_event_started_timestamp\n",
    "\t\t, event_started_timestamp\n",
    "\t\t, next_event_started_timestamp\n",
    "\t\t-- Scenario 1: event starts & ends within the same hour\n",
    "\t\t, EXTRACT('epoch' FROM (next_event_started_timestamp - event_started_timestamp))/3600 AS delta_hours_1\n",
    "\t\t-- Scenario 2: event starts in the current base hour, then ends in another base hour later\n",
    "\t\t, EXTRACT('epoch' FROM (next_base_hour - event_started_timestamp))/3600 AS delta_hours_2\n",
    "\t\t-- Scenario 3: event starts in the current base hour or from a previous base hour, then ends in the current base hour\n",
    "\t\t, EXTRACT('epoch' FROM (LEAST(next_event_started_timestamp, next_base_hour) - event_started_timestamp))/3600 AS delta_hours_3\n",
    "\t\t-- Scenario 4: event from in a previous base hour, then ends in the future base hour (i.e. full hour closure)\n",
    "\t\t-- Not showing here as it is not gonna be covered in this CTE\n",
    "\tFROM delivery_radius_events\n",
    "), delivery_radius_events_processed_further AS (\n",
    "\tSELECT event_id\n",
    "\t\t, delivery_area_id\n",
    "\t\t, base_hour\n",
    "\t\t, next_base_hour\n",
    "\t\t, previous_event_started_timestamp\n",
    "\t\t, event_started_timestamp\n",
    "\t\t, next_event_started_timestamp\n",
    "\t\t, (\n",
    "\t\t\tCASE\n",
    "\t\t\t\t-- Scenario 1: event starts & ends within the same hour\n",
    "\t\t\t\tWHEN base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\t\tAND base_hour = DATE_TRUNC('hour', next_event_started_timestamp)\n",
    "\t\t\t\t\tTHEN 1\n",
    "\t\t\t\t-- Scenario 2: event starts in the current base hour, then ends in another base hour later\n",
    "\t\t\t\tWHEN base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\t\tAND next_event_started_timestamp > next_base_hour\n",
    "\t\t\t\t\tTHEN 2\n",
    "\t\t\t\t-- Scenario 3: event from in a previous base hour, then ends in the current base hour\n",
    "\t\t\t\tWHEN previous_event_started_timestamp < base_hour\n",
    "\t\t\t\t\t\tAND base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\tTHEN 3\n",
    "\t\t\t\t-- Scenario 4 cannot be handled here, to be taken care later\n",
    "\t\t\t\tELSE 0\n",
    "\t\t\tEND\n",
    "\t\t\t ) AS Scenario\n",
    "\t\t, (\n",
    "\t\t\tCASE\n",
    "\t\t\t\t-- Scenario 1: event starts & ends within the same hour\n",
    "\t\t\t\tWHEN base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\t\tAND base_hour = DATE_TRUNC('hour', next_event_started_timestamp)\n",
    "\t\t\t\t\tTHEN delta_hours_1\n",
    "\t\t\t\t-- Scenario 2: event starts in the current base hour, then ends in another base hour later\n",
    "\t\t\t\tWHEN base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\t\tAND next_event_started_timestamp > next_base_hour\n",
    "\t\t\t\t\tTHEN delta_hours_2\n",
    "\t\t\t\t-- Scenario 3: event from in a previous base hour, then ends in the current base hour\n",
    "\t\t\t\tWHEN previous_event_started_timestamp < base_hour\n",
    "\t\t\t\t\t\tAND base_hour = DATE_TRUNC('hour', event_started_timestamp)\n",
    "\t\t\t\t\tTHEN delta_hours_3\n",
    "\t\t\t\t-- Scenario 4 cannot be handled here, to be taken care later\n",
    "\t\t\t\tELSE 0\n",
    "\t\t\tEND\n",
    "\t\t\t ) AS delta_hours\n",
    "\t\t, delta_hours_1\n",
    "\t\t, delta_hours_2\n",
    "\t\t, delta_hours_3\n",
    "\tFROM delivery_radius_events_processed\n",
    "), delivery_radius_events_processed_further_agg_part1 AS (\n",
    "\t-- Scenario 1-3\n",
    "\tSELECT delivery_area_id\n",
    "\t\t, base_hour\n",
    "\t\t, SUM(delta_hours) AS delta_hours\n",
    "\tFROM delivery_radius_events_processed_further\n",
    "\tGROUP BY 1,2\n",
    "\tORDER BY delivery_area_id, base_hour\n",
    "), delivery_radius_events_processed_further_agg_part2 AS (\n",
    "\t-- Scenario 4\n",
    "\tSELECT base.delivery_area_id\n",
    "\t\t, base.base_hour\n",
    "\t\t, 1 AS delta_hours\n",
    "\tFROM base_hours_with_delivery_areas base\n",
    "\tINNER JOIN delivery_radius_events_processed_further events ON base.delivery_area_id = events.delivery_area_id\n",
    "\t\tAND base.base_hour BETWEEN events.event_started_timestamp AND events.next_event_started_timestamp\n",
    "\t    -- The following 2 lines are to filtering double JOINs \n",
    "\t\tAND events.previous_event_started_timestamp < base.base_hour\n",
    "\t\tAND events.next_event_started_timestamp > DATE_ADD(base.base_hour, INTERVAL '1 hour')\n",
    "\tORDER BY base.delivery_area_id, base.base_hour\n",
    "), delivery_radius_events_union_all_agg AS (\n",
    "\tSELECT delivery_area_id\n",
    "\t\t, base_hour\n",
    "\t\t, SUM(delta_hours) AS delta_hours\n",
    "\tFROM (\n",
    "\t\tSELECT *\n",
    "\t\tFROM delivery_radius_events_processed_further_agg_part1\n",
    "\n",
    "\t\tUNION ALL\n",
    "\n",
    "\t\tSELECT *\n",
    "\t\tFROM delivery_radius_events_processed_further_agg_part2\n",
    "\t)\n",
    "\tGROUP BY 1,2\n",
    "\tORDER BY delivery_area_id, base_hour\n",
    ")\n",
    "SELECT base.delivery_area_id\n",
    "\t, base.base_hour\n",
    "\t, COALESCE(agg.delta_hours, 0) AS delta_hours_radius_reduction\n",
    "FROM base_hours_with_delivery_areas base\n",
    "LEFT JOIN delivery_radius_events_union_all_agg agg ON base.base_hour = agg.base_hour\n",
    "\tAND base.delivery_area_id = agg.delivery_area_id\n",
    "ORDER BY base.delivery_area_id, base.base_hour\n",
    "'''\n",
    "\n",
    "df_sum_delivery_radius_reduction = pd.read_sql(sql_sum_delivery_radius_reduction, con = db)\n",
    "\n",
    "df_sum_delivery_radius_reduction.to_sql(\n",
    "    name = 'delivery_radius_reduction',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_sum_delivery_radius_reduction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff111975-e37d-4022-9c7c-4582b36082ac",
   "metadata": {},
   "source": [
    "### Generate an intermedate table for Delivery Radius Modifciation events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a78bcbd-162e-4b34-bb30-b43eb07b96a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_hour</th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>nb_delivery_radius_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-01 12:00:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-02 13:00:00</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-02 14:00:00</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-03 17:00:00</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-05 14:00:00</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            base_hour          delivery_area_id  nb_delivery_radius_modified\n",
       "0 2021-12-01 12:00:00  5cc1b60b034adf90cd8f14dd                            2\n",
       "1 2021-12-02 13:00:00  5cc1b60b034adf90cd8f14dd                            2\n",
       "2 2021-12-02 14:00:00  5d78a7e552dfabd5251dab7b                            2\n",
       "3 2021-12-03 17:00:00  5db02e5d401d690c836b9ead                            3\n",
       "4 2021-12-05 14:00:00  5db02e5d401d690c836b9ead                            2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_delivery_radius_modifications = '''\n",
    "SELECT DATE_TRUNC('hour', event_started_timestamp) AS base_hour\n",
    "\t, delivery_area_id\n",
    "\t, COUNT(*) AS nb_delivery_radius_modified\n",
    "FROM fct.delivery_areas_default_radius_events\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2\n",
    "'''\n",
    "\n",
    "df_delivery_radius_modifications = pd.read_sql(sql_delivery_radius_modifications, con = db)\n",
    "df_delivery_radius_modifications.to_sql(\n",
    "    name = 'delivery_radius_modifications',\n",
    "    schema = 'int',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_delivery_radius_modifications.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d44970-3f6b-4c69-9fa0-0c5a0f13b2ba",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Assumption: \n",
    "1. For the WoW (Week-over-week) comparsion, it is assumed to compare for the same Delivery Area, same hour in the day, and the same day of week. E.g. the number of Purchases from this Monday 1800 - 1900 would only be comparing to the number of Purchases from last Monday 1800 - 1900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc55a60d-f59b-4d17-9fa0-e8652c6f5917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_hour</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>delivery_area_id</th>\n",
       "      <th>nb_purchases</th>\n",
       "      <th>end_amount_with_vat_eur</th>\n",
       "      <th>delta_hours_radius_reduction</th>\n",
       "      <th>nb_delivery_radius_modified</th>\n",
       "      <th>nb_purchases_last_week</th>\n",
       "      <th>end_amount_with_vat_eur_last_week</th>\n",
       "      <th>delta_hours_radius_reduction_last_week</th>\n",
       "      <th>nb_delivery_radius_modified_last_week</th>\n",
       "      <th>nb_purchases_wow_perc</th>\n",
       "      <th>end_amount_with_vat_eur_wow_perc</th>\n",
       "      <th>delta_hours_radius_reduction_wow_perc</th>\n",
       "      <th>nb_delivery_radius_modified_wow_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5db02e5d401d690c836b9ead</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5cc1b60b034adf90cd8f14dd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5d78a7e552dfabd5251dab7b</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            base_hour  hour_of_day  day_of_week          delivery_area_id  \\\n",
       "0 2022-01-01 00:00:00          0.0          6.0  5cc1b60b034adf90cd8f14dd   \n",
       "1 2022-01-01 00:00:00          0.0          6.0  5d78a7e552dfabd5251dab7b   \n",
       "2 2022-01-01 00:00:00          0.0          6.0  5db02e5d401d690c836b9ead   \n",
       "3 2022-01-01 01:00:00          1.0          6.0  5cc1b60b034adf90cd8f14dd   \n",
       "4 2022-01-01 01:00:00          1.0          6.0  5d78a7e552dfabd5251dab7b   \n",
       "\n",
       "   nb_purchases  end_amount_with_vat_eur  delta_hours_radius_reduction  \\\n",
       "0             0                      0.0                           0.0   \n",
       "1             0                      0.0                           0.0   \n",
       "2             0                      0.0                           0.0   \n",
       "3             0                      0.0                           0.0   \n",
       "4             0                      0.0                           0.0   \n",
       "\n",
       "   nb_delivery_radius_modified  nb_purchases_last_week  \\\n",
       "0                            0                     NaN   \n",
       "1                            0                     NaN   \n",
       "2                            0                     NaN   \n",
       "3                            0                     NaN   \n",
       "4                            0                     NaN   \n",
       "\n",
       "   end_amount_with_vat_eur_last_week  delta_hours_radius_reduction_last_week  \\\n",
       "0                                NaN                                     NaN   \n",
       "1                                NaN                                     NaN   \n",
       "2                                NaN                                     NaN   \n",
       "3                                NaN                                     NaN   \n",
       "4                                NaN                                     NaN   \n",
       "\n",
       "   nb_delivery_radius_modified_last_week  nb_purchases_wow_perc  \\\n",
       "0                                    NaN                    0.0   \n",
       "1                                    NaN                    0.0   \n",
       "2                                    NaN                    0.0   \n",
       "3                                    NaN                    0.0   \n",
       "4                                    NaN                    0.0   \n",
       "\n",
       "   end_amount_with_vat_eur_wow_perc  delta_hours_radius_reduction_wow_perc  \\\n",
       "0                               0.0                                    0.0   \n",
       "1                               0.0                                    0.0   \n",
       "2                               0.0                                    0.0   \n",
       "3                               0.0                                    0.0   \n",
       "4                               0.0                                    0.0   \n",
       "\n",
       "   nb_delivery_radius_modified_wow_perc  \n",
       "0                                   0.0  \n",
       "1                                   0.0  \n",
       "2                                   0.0  \n",
       "3                                   0.0  \n",
       "4                                   0.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_sum_purchases_delivery_radius_reduction = '''\n",
    "WITH base_hours_with_delivery_areas AS (\n",
    "\tSELECT base_hours.base_hour\n",
    "\t\t, delivery_areas.delivery_area_id\n",
    "\tFROM int.base_hours\n",
    "\tCROSS JOIN dim.delivery_areas\n",
    "), base_hours_with_metrics AS (\n",
    "\tSELECT base.base_hour\n",
    "\t\t, EXTRACT('hour' FROM base.base_hour) AS hour_of_day\n",
    "\t\t, EXTRACT('dow' FROM base.base_hour) AS day_of_week\n",
    "\t\t, base.delivery_area_id\n",
    "\t\t, COALESCE(sum_purchases.nb_purchases, 0) AS nb_purchases\n",
    "\t\t, COALESCE(sum_purchases.end_amount_with_vat_eur, 0) AS end_amount_with_vat_eur\n",
    "\t\t, COALESCE(delivery_radius_reduction.delta_hours_radius_reduction, 0) AS delta_hours_radius_reduction\n",
    "\t\t, COALESCE(delivery_radius_modifications.nb_delivery_radius_modified, 0) AS nb_delivery_radius_modified\n",
    "\tFROM base_hours_with_delivery_areas base\n",
    "\tLEFT JOIN int.sum_purchases ON base.base_hour = sum_purchases.base_hour\n",
    "\t\tAND base.delivery_area_id = sum_purchases.delivery_area_id\n",
    "\tLEFT JOIN int.delivery_radius_reduction ON base.base_hour = delivery_radius_reduction.base_hour\n",
    "\t\tAND base.delivery_area_id = delivery_radius_reduction.delivery_area_id\n",
    "\tLEFT JOIN int.delivery_radius_modifications ON base.base_hour = delivery_radius_modifications.base_hour\n",
    "\t\tAND base.delivery_area_id = delivery_radius_modifications.delivery_area_id\n",
    "\tORDER BY base.base_hour, base.delivery_area_id\n",
    "), base_hours_with_metrics_and_metrics_from_last_week AS (\n",
    "\tSELECT *\n",
    "\t\t, LAG(nb_purchases) OVER (PARTITION BY delivery_area_id, hour_of_day, day_of_week ORDER BY base_hour) AS nb_purchases_last_week\n",
    "\t\t, LAG(end_amount_with_vat_eur) OVER (delivery_area_dow_hour_window) AS end_amount_with_vat_eur_last_week\n",
    "\t\t, LAG(delta_hours_radius_reduction) OVER (delivery_area_dow_hour_window) AS delta_hours_radius_reduction_last_week\n",
    "\t\t, LAG(nb_delivery_radius_modified) OVER (delivery_area_dow_hour_window) AS nb_delivery_radius_modified_last_week\n",
    "\tFROM base_hours_with_metrics\n",
    "\tWINDOW delivery_area_dow_hour_window AS (\n",
    "\t\tPARTITION BY delivery_area_id, hour_of_day, day_of_week \n",
    "\t\tORDER BY base_hour\n",
    "\t)\n",
    "\tORDER BY base_hour, delivery_area_id\n",
    ")\n",
    "SELECT *\n",
    "  -- To avoid Division by Zero error. Once again, if it is on BigQuery/Snowflake, we could just use Safe Divide functions like `DIV0`\n",
    "    , CASE \n",
    "        WHEN nb_purchases_last_week != 0 \n",
    "                AND nb_purchases_last_week IS NOT NULL\n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (nb_purchases::FLOAT - nb_purchases_last_week)/ nb_purchases_last_week\n",
    "        ELSE 0 \n",
    "      END AS nb_purchases_wow_perc\n",
    "    , CASE \n",
    "        WHEN end_amount_with_vat_eur_last_week != 0 \n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (end_amount_with_vat_eur::FLOAT - end_amount_with_vat_eur_last_week)/ end_amount_with_vat_eur_last_week\n",
    "        ELSE 0 \n",
    "      END AS end_amount_with_vat_eur_wow_perc\n",
    "    , CASE \n",
    "        WHEN delta_hours_radius_reduction_last_week != 0 \n",
    "                AND delta_hours_radius_reduction_last_week IS NOT NULL\n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (delta_hours_radius_reduction::FLOAT - delta_hours_radius_reduction_last_week)/ delta_hours_radius_reduction_last_week\n",
    "        ELSE 0 \n",
    "      END AS delta_hours_radius_reduction_wow_perc\n",
    "    , CASE \n",
    "        WHEN nb_delivery_radius_modified_last_week != 0\n",
    "                AND nb_delivery_radius_modified_last_week IS NOT NULL\n",
    "            -- Small workaround with the data type CASTing issues in PostgreSQL\n",
    "            THEN (nb_delivery_radius_modified::FLOAT - nb_delivery_radius_modified_last_week)/ nb_delivery_radius_modified_last_week\n",
    "        ELSE 0 \n",
    "      END AS nb_delivery_radius_modified_wow_perc\n",
    "FROM base_hours_with_metrics_and_metrics_from_last_week\n",
    "\n",
    "'''\n",
    "\n",
    "df_sum_purchases_delivery_radius_reduction = pd.read_sql(sql_sum_purchases_delivery_radius_reduction, con = db)\n",
    "df_sum_purchases_delivery_radius_reduction.to_sql(\n",
    "    name = 'purchases_delivery_radius_reduction',\n",
    "    schema = 'sum',\n",
    "    con=db,\n",
    "    if_exists = 'replace',\n",
    "    index = False\n",
    ")\n",
    "\n",
    "df_sum_purchases_delivery_radius_reduction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b5efe-005d-41f5-9403-dcb2686b62ab",
   "metadata": {},
   "source": [
    "# Solution Clarifications\n",
    "\n",
    "Thus, in addition to solving the tasks, please also answer the following questions to clarify your solution:\n",
    "* What assumptions about the data have you made to produce the dataset?\n",
    "* Why did you decide to go with this particular approach and what could be the pros and cons of applying it?\n",
    "* How could the solution be improved if given more time and data?\n",
    "* What strategy would you use for updating the dataset from task 2? Consider how often the default radius should be calculated, do we need to truncate the table before updating etc. Please assume that upstream data (i.e. purchases & delivery_radius_log) is streamed to the tables, so changes arrive near real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f92e63-e647-4bec-8503-e6798b79ad73",
   "metadata": {},
   "source": [
    "## What assumptions about the data have you made to produce the dataset?\n",
    "\n",
    "Please see above inline comments in the code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638bca2-31b2-4141-98f0-a868f6993e52",
   "metadata": {},
   "source": [
    "## Why did you decide to go with this particular approach and what could be the pros and cons of applying it?\n",
    "\n",
    "Please see above inline comments in the code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c43cd-9b76-423d-bd4a-aee4d719ca36",
   "metadata": {},
   "source": [
    "## How could the solution be improved if given more time and data?\n",
    "\n",
    "One of the biggest constraints of the current solution is the database. The current selected solution of the database is PostgreSQL, and the only reason of selecting it is the nature of open-source and free. \n",
    "\n",
    "Given the current size of the data (i.e. the 2 CSV  files), it is okay-ish to handle the data transformation with the above approach (e.g. using JOINs instead of Windows Function, processing the data without the required Primary Key and the `updated_at` timestamp, etc.). When the volume of the data increases as the business grows, the performance & cost of all the above queries in PostgreSQL are very likely to degrade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a97cc-d17d-4e08-8fa3-f0fc2b3cda09",
   "metadata": {},
   "source": [
    "## What strategy would you use for updating the dataset from task 2?  Consider how often the default radius should be calculated, do we need to truncate the table before updating etc. Please assume that upstream data (i.e. purchases & delivery_radius_log) is streamed to the tables, so changes arrive near real time.\n",
    "\n",
    "This is a very good question, and it deserves a proper explanation here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42781f-4ed6-4aa2-bc55-078a49a088a4",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "#### Importing columns missing from the giving `delivery_radius_log`\n",
    "\n",
    "As a starter, there are 2 very critical fields/columns missing from the `delivery_radius_log`:\n",
    "1. A Primary Key (e.g. `event_id`)\n",
    "    * A Surrogate Key can still be derived by using both of `delivery_area_id` & `event_started_timestamp`. It is okay in most cases, **but** in certain corner cases (e.g. duplicated events from the upstream application/micro-service, Kafka events replay), even the Surrogate Key would be duplicated.\n",
    "3. TIMESTAMPs indicating when is the event being ingested into the data warehouse (e.g. `created_at`, `updated_at`, or `ingested_at`)\n",
    "\n",
    "Imagine if these 2 columns exist in `delivery_radius_log`, the schema would look like this:\n",
    "* `event_id`\n",
    "* `delivery_area_id`\n",
    "* `delivery_radius_meters`\n",
    "* `event_started_timestamp`\n",
    "* `ingested_at`\n",
    "\n",
    "then, it would be possible to leverage the strength of the data warehouse (e.g. Snowflake, BigQuery) to do something like this:\n",
    "\n",
    "```SQL\n",
    "SELECT * EXCEPT (_row_number)\n",
    "FROM (\n",
    "    SELECT *\n",
    "        , ROW_NUMBER() OVER (PARTITION BY event_id ORDER BY ingested_at DESC) AS _row_number\n",
    "    FROM delivery_radius_log\n",
    ")\n",
    "WHERE _row_number = 1\n",
    "```\n",
    "\n",
    "If this is in place, the data pipeline would still be fairly resilient even if there are duplications from the streaming events (e.g. replaying in Kafka).\n",
    "\n",
    "#### The combine key of the Summary table\n",
    "In the summary table `sum.purchases_delivery_radius_reduction`, the columns being used as combine key are:\n",
    "* `base_hour`\n",
    "* `delivery_area_id`\n",
    "\n",
    "These would be very vital when it comes to refreshing the table.\n",
    "\n",
    "#### Partitioning & Clustering\n",
    "\n",
    "Before actually implementing the dataset, there are MUST-HAVEs to be put in place for the sake of query performance & cost:\n",
    "1. Partitioning (DATE/TIMESTAMP Partitioning in BigQuery, Micro-Partitions in Snowflake)\n",
    "2. Clustering\n",
    "\n",
    "These 2 elements are extremely important when it comes to high-volume & high-velocity data (e.g. tracking vents, or the `delivery_radius_log` in this assignment). Proper Partitioning would help the data warehouse skip irrelevant rows of records which leads to good performance, Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4680257-ceb5-474c-a73c-a53693f64f2b",
   "metadata": {},
   "source": [
    "### Data Updating Strategy for the Task 2 dataset\n",
    "\n",
    "Essentially, the nature of the Task 2 dataset is a Summary table containing pre-computed & pre-aggregated Metrics from both Sales (Purchases & Revenue) and Delivery Radius Events (Radius Reduction & Radius Modifications).\n",
    "\n",
    "#### Factors to consider\n",
    "There are a couple of factors that should be considered regarding deciding the Data Updating Strategy:\n",
    "* Freshness of the upstream data (i.e. `purchases` & `delivery_radius_log`) - and it is known to be near real-time\n",
    "* Business Requirement: Even if it is technically possible, it doesn't mean that it *has to* be delivered on near-realtime as well. At the end of the day, it depends on the business use-case.\n",
    "  * For example, if it is just used for daily reporting/dashboarding, then just a daily refresh would be enough;\n",
    "  * On the other hand, if someone from the business team is actively looking at those numbers literally a few times every hour AND makes decisions based on it, then **maybe** it makes sense to refresh on an hourly basis.\n",
    "    * This sort of business use case needs a more proper tech solution instead of depending on an analytical dataset, with the rationale of the resources required to maintain the SLA and the stability of the pipeline since high-velocity data pipelines tend to be more volatile than slower ones.\n",
    "* The Business Definitions of the metrics:\n",
    "  * The Sales related Metrics (Purchases & Revenue) are rather straightforward, as they often just anchor on the \"base TIMESTAMP\" (e.g. `time_received`) unless there is a special requirement from the business logic\n",
    "  * Yet, the trick lies with the event-based data interval calculation - namely Delivery Radius Reduction. It is tricky as the given definition of the Default Delivery Radius solely depends on the Delivery Radius sticking around for more than 24 hours.\n",
    "    * A better scenario would be whatever application or micro-service is generating those events should make the Default Delivery Radius data available somewhere else (e.g. another Kafka topic) instead of populating everyone in just 1 Kafka topic and ask the Data/BI team to do the transformation.\n",
    "      * The rationale behind is the principle of \"Shift Left\" - doing so at the beginning would just cost 1 Euro, doing so in the middle would cost 10 Euro, and doing it at the end could cost 100+ Euro.\n",
    "\n",
    "\n",
    "### Proposed Strategy\n",
    "Based on the factors mentioned above, here would be the spec of the Proposed update strategy:\n",
    "* Materialization: Incremental\n",
    "  * The TIMESTAMPs from the upstream tables (`purchases` & `delivery_radius_log`) would be required here to implement incremental filtering\n",
    "    * it is also equally important to plan for backfills in the future. Ideally speaking, there should be a condition statement (i.e. `IF`) to check if the current run is an Incremental one. If not, then the TIMESTAMP filter should be removed from the SQL statement to allow the data warehouse to process the whole table\n",
    "  * The combined key of `base_hour` + `delivery_area_id` in the Summary table would leverage to identify which exact record contains new Metrics from the run and hence should be updated as well\n",
    "* Interval: Hourly/Daily (depends on the business use case)\n",
    "  * It is mostly based on the required Dimension being on an hourly basis. Although it is technically possible to make it even more frequent, the cost of the additional volatility and potential confusion for the stakeholders (e.g. very low numbers of the latest record as the hour hasn't been completed yet)\n",
    "* Regular backfills & Re-clustering\n",
    "  * While the Incremental load is good for performance & cost, if there are anomalies of the upstream events (e.g. the events arrive late but with a much older TIMESTAMP value), those records will never be covered by the incremental load\n",
    "  * While a table has records being inserted only for a certain period, the clustering would slowly be messy over time. It might be beneficial to have it re-clustered once a while (e.g. every quarter/half-year, or even on an annual basis)\n",
    "\n",
    "***Notes***\n",
    "* It is not recommended that the pattern of truncating the table first due to resilience consideration.\n",
    "  * Depending on which data warehouse is being used (e.g. Snowflake), there is a chance truncating operations would cause problems. Since truncating and inserting are different queries, if there is a service disruption and somehow makes one of the queries fail, it would result in duplication or unexpected missing records (there is also a risk of having the whole table empty if human error is in the mix)\n",
    "* Last but not least, the use of Incremental Load is a 2-edged sword. It brings the benefit of performance & cost, while it also introduces extra complexity. It is vital to ensure the foundation & tech debt are in a good state to implement this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823f95d-3664-4d90-ab07-b925c31e1d98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "459e304e-dd39-4cad-92d3-9711701d34d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f93c5b-e382-4030-aabc-8490f58b2655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
